\chapter{Numerics}

To test the performance of the Riemannian Trust-Region Symmetric Rank-One method, implemented in the package \lstinline!Manopt.jl!, we consider the Rayleigh quotient minimization problem on the sphere $\mathbb{S}^{n-1}$. For a symmetric matrix $A \in \mathbb{R}^{n \times n}$, the unit-norm eigenvector, $v \in \mathbb{R}^n$, corresponding to the smallest eigenvalue, defines the two global minima, $\pm v$, of the Rayleigh quotient  
\begin{equation}\label{RayleighQuotient}
    \begin{split}
        f \colon \; \mathbb{S}^{n-1} & \to \mathbb{R} \\
        x & \mapsto x^{\mathrm{T}} A x 
    \end{split}
\end{equation}   
with its Riemannian gradient 
\begin{equation*}
    \operatorname{grad} f(x) = 2(Ax - x x^{\mathrm{T}} A x),
\end{equation*}
and its Hessian operator
\begin{equation}\label{RayleighHessian}
    \operatorname{Hess} f(x) [\xi_x] = 2 (A \xi_x - x x^{\mathrm{T}} A \xi_x - \xi_x x^{\mathrm{T}} A x - x x^{\mathrm{T}} \xi_x x^{\mathrm{T}} A x).
\end{equation}
For the retraction $\retract{\cdot}(\cdot)$ on $\mathbb{S}^{n-1}$ we use the so-called retraction by projection, which is given by
\begin{equation*}
    \retract{x}(\xi_x) = \frac{x + \xi_x}{\lVert x + \xi_x \rVert_2},
\end{equation*}
and as isometric vector transport $\mathrm{T}^S$ on $\mathbb{S}^{n-1}$ we use the prarallel transport, which is given by 
\begin{equation*}
    \parallelTransportDir{x}{\eta_x}(\xi_x) = \xi_x - \frac{2 y^{\mathrm{T}} \xi_x}{\lVert x + y \rVert^{2}_2} (x + y), \text{ where } y = \retract{x}(\eta_x).
\end{equation*}
To apply the RTR-SR1 method, implemented in \lstinline!Manopt.jl!, to the optimization problem defined by the cost function \cref{RayleighQuotient} for $n=500$, \cref{RayleighCode} must be executed in Julia. The problem is defined by setting \lstinline!A_symm = ( A + A' ) / 2!, where the elements of \lstinline!A! are drawn from the standard normal distribution using Juliaâ€™s \lstinline!randn(n,n)! with seed \lstinline!42!. With \lstinline!random_point(M)!, a random point is generated on the given manifold \lstinline!M!. The stopping criterion requires to abort the method, that either the norm of the gradient is less than $10^{-6}$ or $500$ iterations have been run. \\
\begin{lstlisting}[mathescape, caption={The Rayleigh quotient minimization experiment in Julia for $n = 500$.}, label={RayleighCode}]
    using Manopt, Manifolds, Random
    Random.seed!(42)

    n = 50
    A = randn(n,n)
    A = ( A + A' ) / 2
    F(::Sphere, p::Array{Float64,1}) = p' * A * p
    gradF(::Sphere, p::Array{Float64,1}) = 2 * (A * p - p * p' * A * p)
    HessF(::Sphere, p::Array{Float64,1}, X::Array{Float64,1}) = 2 * (A * X - p * p' * A * X - X * p' * A * p - p * p' * X * p' * A * p)
    M = Sphere(n - 1)
    x = random_point(M)

    trust_regions!(
    M,
    F,
    gradF,
    ApproxHessianSymmetricRankOne(M, x, gradF; nu = sqrt(eps(Float64))),
    x;
    $\theta$ = 0.1,
    $\kappa$ = 0.9,
    stopping_criterion=StopWhenAny(
    StopAfterIteration(500), StopWhenGradientNormLess(10^(-6))
    ),
    retraction_method = ProjectionRetraction(),
    trust_region_radius = 1.0
    )
\end{lstlisting}
To test the performance, we execute an experiment in Julia, where \cref{RayleighCode} depending on the parameter \lstinline!n! is used. We compare the average number of iterations per call, the average time needed per call and the average number of thousand allocations per call of the RTR-Newton method, which uses the true Hessian operator \cref{RayleighHessian} as quadratic term, and the RTR-SR1 method. Both methods use as inner iteration for solving the trust-region subproblem \cref{Riemanniantrsubproblem} a generalization of the truncated conjugate-gradient method for Riemannian manifolds, which follows the methodology of \cite[Algorithm~11]{AbsilMahonySepulchre:2008}. The parameters $\theta$ and $\kappa$ in the inner iteration stopping criterion \cite[(7.10)]{AbsilMahonySepulchre:2008} are set to $0.1, 0.9$ for RTR-SR1 and to $1, 0.1$ for RTR-Newton. The initial trust-region radius $\Delta_0$ is set to $1$, $r$ is the square root of machine epsilon, $\rho^{\prime}$ is set to $0.1$, $\tau_1$ to $0.25$, and $\tau_2$ to $2$. \\
The numerical experiment is implemented in the toolbox \lstinline!Manopt.jl!. It runs on a Lenovo ThinkPad L490, 64 bit Windows system, 1.8 Ghz Intel Core i7-8565U, 32 GB RAM, with Julia 1.6.

\begin{table}[H]
    \centering
        \begin{tabular}{lcccccc}
            \toprule
            Manifold & \multicolumn{2}{c}{$\mathbb{S}^{49}$}& \multicolumn{2}{c}{$\mathbb{S}^{99}$} & \multicolumn{2}{c}{$\mathbb{S}^{199}$}\\ 
            \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
            Quadratic term & Newton & SR1 & Newton & SR1 & Newton & SR1 \\ 

            $\varnothing$ Time in ms & $4.5$ & $17.0$ & $54.2$ & $80.2$ & $245.5$ & $269.4$\\ 
            $\varnothing$ Iterations & $9.2$ & $41.5$ & $10.2$ & $70.1$ & $10.5$ & $83.1$ \\
            $\varnothing$ k Allocations& $3.7$ & $23.0$ & $4.9$ & $35.3$ & $5.9$ & $52.5$ \\
            \bottomrule
        \end{tabular}
    \caption{Comparison of the Riemannian Trust-Region methods.} \label{tab:Results}
\end{table}

\cref{tab:Results} contains the results of both Riemannian Trust-Region methods on $\mathbb{S}^{49}$, $\mathbb{S}^{99}$ and $\mathbb{S}^{199}$. For the measurement of the average time and the average number of thousand allocations, we used the package \lstinline!BenchmarkTools.jl!. The measurement was done with a benchmark of $30$ samples and $10$ evaluation per sample using a random point on the manifold. The iterations are the average of the measured iterations from $10$ random runs. \\
In the results from \cref{tab:Results} it is immediately apparent that the Riemannian Trust-Region Newton method is the clear winner. In all three criteria, this method achieves smaller and therefore better numbers than the SR1 variant. From this it can be concluded that the RTR-Newton method is the apriori method of choice over the RTR-SR1 method, if the Hessian operator is avaiable. \\
Regarding the increased number of iterations, we can fall back on the local convergence analysis (see \cref{RiemannianLocalConvergence}), which tells us that the RTR-SR1 method is slower than the RTR-Newton method whose convergence rate is q-superlinear (see \cite[Theorem~4.13]{AbsilBakerGallivan:2007}). It shows that exploiting the Hessian operator positively influences rate ofconvergence. \\ 
On average, the RTR-SR1 method has six times more allocations than the RTR-Newton method. On the one hand, this is due to the fact that the RTR-SR1 method has not yet been optimized in terms of efficiency, since our initial goal was to make this method work, and on the other hand, because the way we store, apply and update the operator $\mathcal{H}^{\mathrm{SR1}}_k$ requires several intermediate steps in code and thus more resources. \\
At this stage, the operator $\mathcal{H}^{\mathrm{SR1}}_k \colon \tangent{x_k} \to \tangent{x_k}$ is stored as a symmetric matrix $H^{\mathrm{SR1}}_k \in \mathbb{R}^{n \times n}$ (where $n$ is the dimension of the manifold and therefore also of each tangent space) with respect to an orthonormal basis on the tangent space $\{ b_i \}^{n}_{i=1} \subset \tangent{x_k}$, $g_{x_k}(b_i, b_j) = \delta_{i,j}$. This means that each time we apply the operator $\mathcal{H}^{\mathrm{SR1}}_k$ to a tangent vector $\eta_{x_k} \in \tangent{x_k}$, we decompose the tangent vector $\eta_{x_k}$ into its real-valued coordinates $\hat{\eta}_{x_k} \in \mathbb{R}^n$ with respect to the orthonormal basis $\{ b_i \}^{n}_{i=1}$, apply the matrix $H^{\mathrm{SR1}}_k$ on the real-valued vector $\hat{\eta}_{x_k}$, i.e. $\hat{\xi}_{x_k} = H^{\mathrm{SR1}}_k \hat{\eta}_{x_k}$, and construct a tangent vector $\xi_{x_k} \in \tangent{x_k}$ from $\hat{\xi}_{x_k}$ using the orthonormal basis $\{ b_i \}^{n}_{i=1}$, i.e. $\xi_{x_k} = (\hat{\xi}_{x_k})_{1} b_1 + \cdots + (\hat{\xi}_{x_k})_{n} b_n \in \tangent{x_k}$. \\
This implementation makes it easier to store and update the operator $\mathcal{H}^{\mathrm{SR1}}_k$. If we update the operator $\mathcal{H}^{\mathrm{SR1}}_k \rightarrow \widetilde{\mathcal{H}}^{\mathrm{SR1}}_{k+1}$, then we update the matrix $H^{\mathrm{SR1}}_k \rightarrow H^{\mathrm{SR1}}_{k+1}$ with the Euclidean update formula \cref{directSR1formula} using the real-valued coordinates $\hat{s}_k, \hat{y}_k \in \mathbb{R}^n$ of the tangent vectors $s_k, y_k \in \tangent{x_k}$, and when we transport the operator, we actually transport the orthonormal basis into the new tangent space, i.e. $\tangent{x_k} \supset \{ b_i \}^{n}_{i=1} \rightarrow \mathrm{T}^{S}_{x_k, s_k}(\{ b_i \}^{n}_{i=1}) \subset \tangent{x_{k+1}}$. \\ 
Especially generating the real-valued coordinates with respect to the orthonormal basis, which occurs both when applying (which is frequent in the tCG-method) and updating the operator, can generate a high number of allocations. \\

% Hihger number of allocations

The increased average time per call of the RTR-SR1 method can be attributed on the one hand to the higher average number of iterations and on the other hand to the higher average number of thousand allocations. It is interesting to note, however, that although the RTR-SR1 method goes through several steps with each application of the operator $\mathcal{H}^{\mathrm{SR1}}_k$, it still takes less time per iteration. Looking at the ratio of average time of a run per average number of iterations of a run, the RTR-SR1 method is faster than the RTR-Newton method. This is especially evident in the higher dimensional manifolds. On $\mathbb{S}^{99}$, the RTR-SR1 method takes $1.14$ ms per iteration and the RTR-Newton method takes $5.31$ ms per iteration on average. On $\mathbb{S}^{199}$, the RTR-SR1 method takes $3.24$ ms per iteration and the RTR-Newton method takes $23.38$ ms per iteration on average. \\
Therefore, it can be concluded that there is a chance that the RTR-SR1 method can take less time than the RTR-Newton method, especially for large manifolds, if the average number of iterations and thousand allocations can be reduced (as much as possible).
