\chapter{Numerics}

To test the performance of the Riemannian Trust-Region Symmetric Rank-One method, implemented in the package \lstinline!Manopt.jl!, we consider the Rayleigh quotient minimization problem on the sphere $\mathbb{S}^{n-1}$. For a symmetric matrix $A \in \mathbb{R}^{n \times n}$, the unit-norm eigenvector, $v \in \mathbb{R}^n$, corresponding to the smallest eigenvalue, defines the two global minima, $\pm v$, of the Rayleigh quotient  
\begin{equation}\label{RayleighQuotient}
    \begin{split}
        f \colon \ \mathbb{S}^{n-1} & \to \mathbb{R} \\
        x & \mapsto x^{\mathrm{T}} A x 
    \end{split}
\end{equation}   
with its Riemannian gradient 
\begin{equation*}
    \operatorname{grad} f(x) = 2(Ax - x x^{\mathrm{T}} A x),
\end{equation*}
and its Hessian operator
\begin{equation}\label{RayleighHessian}
    \operatorname{Hess} f(x) [\xi_x] = 2 (A \xi_x - x x^{\mathrm{T}} A \xi_x - \xi_x x^{\mathrm{T}} A x - x x^{\mathrm{T}} \xi_x x^{\mathrm{T}} A x).
\end{equation}
As retraction $\retractionSymbol \colon \ \tangent{x}[\mathbb{S}^{n-1}] \to \mathbb{S}^{n-1}$ we use the so-called retraction by projection, which is given by
\begin{equation*}
    \retract{x}(\xi_x) = \frac{x + \xi_x}{\lVert x + \xi_x \rVert_2},
\end{equation*}
and as isometric vector transport $\mathrm{T}^S \colon \ \tangent{x}[\mathbb{S}^{n-1}] \to \tangent{\retract{x}(\eta_x)}[\mathbb{S}^{n-1}]$ we use the so-called prarallel transport (see \cite[p.~104]{AbsilMahonySepulchre:2008}), which is given by 
\begin{equation*}
    \parallelTransportDir{x}{\eta_x}(\xi_x) = \xi_x - \frac{2 y^{\mathrm{T}} \xi_x}{\lVert x + y \rVert^{2}_2} (x + y), \text{ where } y = \retract{x}(\eta_x).
\end{equation*}
To apply the RTR-SR1 method, which is implemented in \lstinline!Manopt.jl! and follows the heuristic given by \cref{RTR-SR1Method}, to the optimization problem defined by the cost function \cref{RayleighQuotient} for $n=50$, \cref{RayleighCode} must be executed in Julia. The problem is defined by setting \lstinline!A_symm = ( A + A' ) / 2!, where the elements of \lstinline!A! are drawn from the standard normal distribution using Juliaâ€™s \lstinline!randn(n,n)! with seed \lstinline!42!. With \lstinline!random_point(M)!, a random point on the manifold \lstinline!M = Sphere(n-1)! is generated. The stopping criterion of the RTR-SR1 method requires that either the norm of the current gradient is less than $10^{-6}$ or $500$ iterations have been run. \\

\newpage

\begin{lstlisting}[mathescape, caption={The Rayleigh quotient minimization experiment in Julia for $n = 500$.}, label={RayleighCode}]
    using Manopt, Manifolds, Random
    Random.seed!(42)

    n = 50
    A = randn(n,n)
    A = ( A + A' ) / 2
    F(::Sphere, p::Array{Float64,1}) = p' * A * p
    gradF(::Sphere, p::Array{Float64,1}) = 2 * (A * p - p * p' * A * p)
    HessF(::Sphere, p::Array{Float64,1}, X::Array{Float64,1}) = 2 * (A * X - p * p' * A * X - X * p' * A * p - p * p' * X * p' * A * p)
    M = Sphere(n - 1)
    x = random_point(M)

    trust_regions(
    M,
    F,
    gradF,
    ApproxHessianSymmetricRankOne(M, x, gradF; nu=sqrt(eps(Float64))),
    x;
    $\theta$ = 0.1,
    $\kappa$ = 0.9,
    stopping_criterion=StopWhenAny(
    StopAfterIteration(500), StopWhenGradientNormLess(10^(-6))
    ),
    retraction_method = ProjectionRetraction(),
    trust_region_radius = 1.0
    )
\end{lstlisting}
To test the performance, we execute an experiment in Julia, where \cref{RayleighCode} depending on the parameter \lstinline!n! is used. We compare the average number of iterations per call, the average time needed per call and the average number of thousand allocations per call of the the RTR-SR1 method, which is applied by entering \lstinline!ApproxHessianSymmetricRankOne(M, x, gradF; nu=sqrt(eps(Float64)))! as the fourth necessary input parameter in \lstinline!trust_regions()!, whereby the manifold \lstinline!M!, the initial iterate \lstinline!x! and the gradient \lstinline!gradF! must be passed as necessary input parameters and we can enter the constant for the safeguard \cref{RiemannianSafeguard} via the optional input parameter \lstinline!nu!, by default this is set to $-1$ which means that the update \cref{RiemannianDirectSR1formula} will be executed in every iteration, with the results generated by the RTR-Newton method, which uses the true Hessian operator \cref{RayleighHessian} as quadratic term, i.e. as fourth necessary input parameter in \lstinline!trust_regions()! would be passed \lstinline!HessF!. Both methods use as inner iteration method for solving the trust-region subproblem \cref{Riemanniantrsubproblem} a implemented version of generalization of the truncated conjugate-gradient method for Riemannian manifolds (see \cite[Algorithm~11]{AbsilMahonySepulchre:2008}). The parameters $\theta$ and $\kappa$ of the stopping criterion \cite[(7.10)]{AbsilMahonySepulchre:2008} for the inner iteration method are set to $0.1, 0.9$ for RTR-SR1 and to $1.0, 0.1$ for RTR-Newton. The initial trust-region radius $\Delta_0$ is set to $1$ (optional input parameter \lstinline!trust_region_radius!), $r$ is the square root of machine epsilon (optional input parameter \lstinline!nu!), $\rho^{\prime}$ is set to $0.1$, $\tau_1$ to $0.25$, and $\tau_2$ to $2$. \\
The numerical experiment is implemented in the toolbox \lstinline!Manopt.jl!. It runs on a Lenovo ThinkPad L490, 64 bit Windows system, 1.8 Ghz Intel Core i7-8565U, 32 GB RAM, with Julia 1.6.
\begin{table}[H]
    \centering
        \begin{tabular}{lcccccc}
            \toprule
            Manifold & \multicolumn{2}{c}{$\mathbb{S}^{49}$}& \multicolumn{2}{c}{$\mathbb{S}^{99}$} & \multicolumn{2}{c}{$\mathbb{S}^{199}$}\\ 
            \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
            Quadratic term & Newton & SR1 & Newton & SR1 & Newton & SR1 \\ 

            $\varnothing$ Time in ms & $4.5$ & $17.0$ & $54.2$ & $80.2$ & $245.5$ & $269.4$\\ 
            $\varnothing$ Iterations & $9.2$ & $41.5$ & $10.2$ & $70.1$ & $10.5$ & $83.1$ \\
            $\varnothing$ k Allocations& $3.7$ & $23.0$ & $4.9$ & $35.3$ & $5.9$ & $52.5$ \\
            \bottomrule
        \end{tabular}
    \caption{Comparison of the Riemannian Trust-Region methods.} \label{tab:Results}
\end{table}
\cref{tab:Results} contains the results of both Riemannian Trust-Region methods on $\mathbb{S}^{49}$, $\mathbb{S}^{99}$ and $\mathbb{S}^{199}$. For the measurement of the average time and the average number of thousand allocations we used the package \lstinline!BenchmarkTools.jl!. The measurement was done with a benchmark of $30$ samples and $10$ evaluation per sample using a random point on the manifold. The iterations are the average of the measured iterations from $10$ random runs. \\
In the results from \cref{tab:Results} it is immediately apparent that the Riemannian Trust-Region Newton method is in all three criteria the clear winner. From this it can be concluded that the RTR-Newton method is the apriori method of choice over the RTR-SR1 method, if the Hessian operator is avaiable. \\
Regarding the increased number of iterations, we can fall back on the local convergence analysis (see \cref{RiemannianLocalConvergence}), which tells us that the RTR-SR1 method is slower than the RTR-Newton method whose convergence rate is q-superlinear (see \cite[Theorem~4.13]{AbsilBakerGallivan:2007}). This result is not surprising, since the Hessian operator incorporates more information about the curvature of the objective function than the approximation with the SR1 update, which shows that exploiting the Hessian operator influences the rate of convergence positively. \\ 
On average, the RTR-SR1 method has six times more allocations than the RTR-Newton method. On the one hand, this is due to the fact that the implemented RTR-SR1 method has not yet been optimized in terms of efficiency, since the initial goal was to make this method work, and on the other hand, because the way the method stores, applies and updates the operator $\mathcal{H}^{\mathrm{SR1}}_k$ requires several intermediate steps and thus more resources. \\
At this stage, the operator $\mathcal{H}^{\mathrm{SR1}}_k \colon \ \tangent{x_k} \to \tangent{x_k}$ is stored as a symmetric matrix $H^{\mathrm{SR1}}_k \in \mathbb{R}^{n \times n}$ (where $n$ is the dimension of the manifold and tangent space) with respect to an orthonormal basis on the tangent space $\{ b_i \}^{n}_{i=1} \subset \tangent{x_k}$. \\
That means, each time the operator $\mathcal{H}^{\mathrm{SR1}}_k$ is applied to a tangent vector $\eta_{x_k} \in \tangent{x_k}$, the tangent vector $\eta_{x_k}$ is decomposed into its real-valued coordinates $\hat{\eta}_{x_k} \in \mathbb{R}^n$ with respect to the orthonormal basis $\{ b_i \}^{n}_{i=1}$, the matrix $H^{\mathrm{SR1}}_k$ is applied on the real-valued vector $\hat{\eta}_{x_k}$, i.e. $\hat{\xi}_{x_k} = H^{\mathrm{SR1}}_k \hat{\eta}_{x_k}$, and a tangent vector $\xi_{x_k} \in \tangent{x_k}$ is constructed from $\hat{\xi}_{x_k}$ using the orthonormal basis $\{ b_i \}^{n}_{i=1}$, i.e. $\xi_{x_k} = (\hat{\xi}_{x_k})_{1} b_1 + \cdots + (\hat{\xi}_{x_k})_{n} b_n \in \tangent{x_k}$. \\
This implementation makes it easier to store and update the operator $\mathcal{H}^{\mathrm{SR1}}_k$. If the operator $\mathcal{H}^{\mathrm{SR1}}_k$ is updated, then the matrix $H^{\mathrm{SR1}}_k$ is updated with the Euclidean SR1 update formula, \cref{directSR1formula}, using the real-valued coordinates $\hat{s}_k, \hat{y}_k \in \mathbb{R}^n$ of the tangent vectors $s_k, y_k \in \tangent{x_k}$, and when the operator is transported to $\tangent{x_{k+1}}$, the orthonormal basis is transported into the new tangent space, i.e. $\tangent{x_k} \supset \{ b_i \}^{n}_{i=1} \rightarrow \mathrm{T}^{S}_{x_k, s_k}(\{ b_i \}^{n}_{i=1}) \subset \tangent{x_{k+1}}$. \\ 
Especially generating the real-valued coordinates of the tangent vectors with respect to the orthonormal basis, which occurs both when applying (which is frequent in the tCG-method) and updating the operator, can generate a high number of allocations. \\
The increased average time per call of the RTR-SR1 method can be attributed on the one hand obviously to the higher average number of iterations and on the other hand to the higher average number of thousand allocations. It is interesting to note, however, that the RTR-SR1 method takes less time per iteration than the RTR-Newton. Looking at the ratio of average time per call by average number of iterations per call, we see that on average a single iteration of the RTR-SR1 method takes less time and is therefore faster. This is especially evident in the higher dimensions. On $\mathbb{S}^{99}$, the RTR-SR1 method takes $1.14$ ms per iteration and the RTR-Newton method takes $5.31$ ms per iteration on average. On $\mathbb{S}^{199}$, the RTR-SR1 method takes $3.24$ ms per iteration and the RTR-Newton method takes $23.38$ ms per iteration on average. \\

This experiment shows that the RTR-SR1 method cannot compete with the RTR-Newton method for the optimization problem defined by the cost function \cref{RayleighQuotient}. Nevertheless there is a possibility that the RTR-SR1 method can be the method of choice for high-dimensional optimization problems on Riemannian manifolds if the Hessian operator is not available or is computationally too costly, provided that the average number of iterations and thousand allocations can be reduced (as much as possible).
