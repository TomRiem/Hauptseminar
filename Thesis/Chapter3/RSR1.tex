\section{Riemannian Trust-Region Symmetric Rank-One Method}

%  models, trust-region subproblem, general Trust-Region method, Vector transport, flat, quasi-Newton operator, SR1 Update (unique), safeguard, algorithm, convergence

We will first look at Trust-Region methods on Riemannian manifolds in general. To do this, we must construct a quadratic model for our objective function, which is defined on the Riemannian manifold M. But this is not, in general, a Euclidean space, which makes the situation quite difficult. To avoid this problem, we choose a retraction on the Riemannian manifold, which provides a way to pull back the cost function on the manifold to a cost function on the tangent space. \\
Given a cost function $f \colon \mathcal{M} \to \mathbb{R}$ and a current iterate $x_k \in \mathcal{M}$, we use $\retract{x_k}$ to locally map the minimization problem for $f$ on $\mathcal{M}$ into a minimization problem for the pullback $\hat{f}_{x_k}$ of $f$ under $\retract{x_k}$ on the tangent space $\tangent{x_k}$,
\begin{align*}
    \hat{f}_{x_k} \colon \tangent{x_k} & \to \mathbb{R} \\
    \xi_{x_k} & \mapsto \hat{f}_{x_k} (\xi_{x_k}) = f(\retract{x_k}(\xi_{x_k})).
\end{align*}
The Riemannian metric turns the tangent space $\tangent{x_k}$ into a Euclidean space endowed with the inner product $g_{x_k}(\cdot, \cdot)$, which enables us to construct a order-2 model of the pullback $\hat{f}_{x_k}$:
\begin{align*}
    \hat{m}_k( s ) & = \hat{f}_{x_k}(0_{x_k}) + \mathrm{D} \, \hat{f}_{x_k}(0_{x_k}) [s] + \frac{1}{2} \mathrm{D} \, \hat{f}_{x_k}(0_{x_k}) [s, s] \\
    & = f(x_k) + g_{x_k}(\operatorname{grad} f(x_k), s ) + \frac{1}{2} g_{x_k}( s, \operatorname{Hess} \hat{f}_{x_k}(0_{x_k}) [s]).
\end{align*}
By \cref{Retraction} we get $\hat{f}_{x_k}(0_{x_k}) = f(\retract{x_k}(0_{x_k})) = f(x_k)$ and since the local rigidity condition \cref{LocalRigidity} holds, we have $\mathrm{D} \, \hat{f}_{x_k}(0_{x_k}) = \mathrm{D} \, f(x_k)$, hence $\operatorname{grad} \hat{f}_{x_k}(0_{x_k}) = \operatorname{grad} f(x_k)$. \\
Finally, we “push forward” the model $\hat{m}_k(s)$ through the retraction $\retract{x_k}$, i.e.
\begin{equation}\label{RiemannianModel}
    m_k(s) = \hat{m}_k(s) \circ \inverseRetract{x_k}
\end{equation}
with
\begin{equation}\label{RiemannianModelTangent}
    \hat{m}_k( s ) = f(x_k) + g_{x_k}(\operatorname{grad} f(x_k), s ) + \frac{1}{2} g_{x_k}( s, \operatorname{Hess} f(x_k) [s]),
\end{equation}
where the quadratic term is given by the Riemannian Hessian of the objective function $f$. \\
In general, the model \cref{RiemannianModel} using \cref{RiemannianModel} is only order 1 because $\operatorname{Hess} \hat{f}_{x_k}(0_{x_k}) \neq \operatorname{Hess} f(x_k)$. Since the Hessian operator of the pullback depends among other things on the chosen retraction, further conditions must be imposed on the retraction so that this equality holds. However, for any retraction, $\operatorname{Hess} \hat{f}_{x^*}(0_{x^*}) \neq \operatorname{Hess} f(x^*)$ holds, when $x^*$ is a stationary point of $f$, i.e. $\operatorname{grad} f(x^*) = 0_{x^*}$ \cite[p.~138-139]{AbsilMahonySepulchre:2008}. \\
However, this is of no importance for now, since we only consider quadratic models of the pullback $\hat{f}_{x_k}$ in general, which means 
\begin{equation}\label{RiemannianQuadraticModel}
    \hat{m}_k( s ) = f(x_k) + g_{x_k}(\operatorname{grad} f(x_k), s ) + \frac{1}{2} g_{x_k}( s, \mathcal{H}_k [s]),
\end{equation}
where $\mathcal{H}_k$ is some linear self-adjoint operator on $\tangent{x_k}$. This model \cref{RiemannianQuadraticModel} is easier to handle than the pullback $\hat{f}_{x_k}$ and its purpose is to approximate $\hat{f}_{x_k}$ within a suitable neighbourhood of the zero tangent vector $0_{x_k} \in \tangent{x_k}$ which we refer to as the trust-region. By using the norm induced by the Riemannian metric, we can define the trust-region as the set of all points
\begin{equation}\label{RiemannianTrustRegion}
    \{ s \in \tangent{x_k} \colon \ \lVert s \rVert_{x_k} \leq \Delta_k \}
\end{equation}
where $\Delta_k > 0$ is the trust-region radius. We point out that our trust-region now consists not of all points on the manifold which have the distance less or equal $\Delta_k$ to the current iterate $x_k$, but of all tangent vectors in tangent space of the current iterate $\tangent{x_k}$ whose length is less or equal $\Delta_k$. The chosen retraction $\retractionSymbol$ defines for any iterate $x_k \in \mathcal{M}$, a one-to-one correspondence $\retract{x_k}$ between a neighborhood of $x_k$ in $\mathcal{M}$ and a neighborhood of $0_{x_k}$ in the tangent space $\tangent{x_k}$ (we remember $\retract{x_k}(0_{x_k}) = x_k$) \cite[p.~304]{AbsilBakerGallivan:2007}. But the chosen retraction $\retractionSymbol$ applied to a tangent vector $\xi_{x_k} \in \tangent{x_k}$ generally does not lead to a point $\retract{x_k}(\xi_{x_k})$ whose distance to the starting point $x_k$ is equal to the norm of the tangent vector $\xi_{x_k}$, i.e. $\operatorname{dist}(x_k, \retract{x_k}(\xi_{x_k})) \neq \lVert \xi_{x_k} \rVert_{x_k}$. \\
Next, we compute the step $s_k \in \tangent{x_k}$ as an (approximate) solution of the trust-region subproblem given by the model \cref{RiemannianQuadraticModel} and the trust-region \cref{RiemannianTrustRegion}, i.e.
\begin{equation}\label{Riemanniantrsubproblem}
    s_k = \arg \min_{\lVert s \rVert_{x_k} \leq \Delta_k} \hat{m}_k( s ) \in \tangent{x_k}.
\end{equation}
Since $\tangent{x_k}$ is a Euclidean space, it is possible to adapt classical methods in $R^n$ to compute a approximate minimizer of the trust-region subproblem \cref{Riemanniantrsubproblem} \cite[p.~304]{AbsilBakerGallivan:2007}. A possible method for this is given by \cite[Algorithm~11]{AbsilMahonySepulchre:2008} which is based on a generalization of the truncated conjugate-gradient method into the Riemannian setup. \\
This minimizer $s_k$ is then retracted back from $\tangent{x_k}$ to $\mathcal{M}$, i.e.
\begin{equation*}
    \widetilde{x}_{k+1} = \retract{x_k}(s_k).
\end{equation*}
This point is a candidate for the new iterate $x_{k+1}$. The decisions on accepting or rejecting the candidate $\widetilde{x}_{k+1}$ and on selecting the new trust-region radius $\Delta_k$ are based on the quotient
\begin{equation*}
    \rho_k = \frac{f(x_k) - f(x_k + s_k)}{m_k(0) - m_k(s_k)} = \frac{f(x_k) - f(\widetilde{x}_{k+1})}{m_k(0) - m_k(s_k)},
\end{equation*}



It is this “lift-solve-retract” procedure that distinguishes the proposed RTR
approach from the standard trust-region methods; the standard methods, since they
live in Rn, only require the “solve” part. On a manifold, lifting the cost function
makes it possible to locally fall back to a friendly Euclidean world (the tangent
space Tx M) where classical techniques can be applied, and the retraction brings
the result back to the manifold. A difficulty, from an analysis perspective, is that
the RTR method does not deal with a unique cost function (as in the classical case),
but rather with a succession of different lifted cost functions fxk , where xk is the
kth iterate. A main contribution of this paper is to show that, under reasonable
conditions, the nice properties of the standard trust-region schemes are preserved
in their Riemannian generalizations (see Section 4).



Here $\mathcal{H}_k [\cdot]$ tries to approximate the action of the Riemannian Hessian $\operatorname{Hess} f(x_k) [\cdot]$ (see \cite[Definition~5.5.1]{AbsilMahonySepulchre:2008}) and $\mathcal{B}_k [\cdot]$ approximates the action of ${\operatorname{Hess} f(x_k)}^{-1} [\cdot]$. At the end of each iteration, these operators are updated to new operators $\mathcal{H}_{k+1}, \mathcal{B}_{k+1} \colon \; \tangent{x_{k+1}} \to \tangent{x_{k+1}}$ on the upcoming tangent space $\tangent{x_{k+1}}$. It is required that these satisfy some form of the Riemannian quasi-Newton equation. 

By introducing the tangent vector $s_k = \parallelTransportDir{x_k}{\alpha_k \eta_k}(\alpha_k \eta_k) \in \tangent{x_{k+1}}$, which represents the connection between the two iterates $x_k$ and $x_{k+1}$, and the tangent vector $y_k = \operatorname{grad} f(x_{k+1}) - \parallelTransportDir{x_k}{\alpha_k \eta_k}(\operatorname{grad} f(x_k)) \in \tangent{x_{k+1}}$, which represents the difference of the gradients in $\tangent{x_{k+1}}$, the Riemannian quasi-Newton equation is given by 
\begin{equation*}
    \mathcal{H}_{k+1} [s_k] = y_k 
\end{equation*}

To be able to create rank-one operators, we introduce the musical isomorphism $\flat \colon \; \tangent{x_{k+1}} \ni \eta_{x_{k+1}} \mapsto \eta^{\flat}_{x_{k+1}} \in \cotangent{x_{k+1}}$ (see \cite[p.~6]{BergmannHerzogLouzeiroSilvaTenbrinckVidalNunez:2020:1}). Put simply, it means: $\eta^{\flat}_{x_{k+1}} \in \cotangent{x_{k+1}}$ represents the flat of $\eta_{x_{k+1}} \in \tangent{x_{k+1}}$, i.e., $\eta^{\flat}_{x_{k+1}} \colon \; \tangent{x_{k+1}} \to \mathbb{R}, \;  \xi_{x_{k+1}} \mapsto \eta^{\flat}_{x_{k+1}}[\xi_{x_{k+1}}] = g_{x_{k+1}} (\eta_{x_{k+1}}, \xi_{x_{k+1}})$. This generalizes the notion of the transpose. As noted earlier, there are several quasi-Newton update formulae for operators that have been generalized from the Euclidean case to the Riemannian setup. 

\begin{equation}\label{Riemanniantrsubproblem}
    s_k = \arg \min_{\lVert s \rVert_{x_k} \leq \Delta_k} m_k( s ) = \arg \min_{\lVert s \rVert_{x_k} \leq \Delta_k} f(x_k) + g_{x_k}(\operatorname{grad} f(x_k), s ) + \frac{1}{2} g_{x_k}( s , \mathcal{H}_k [ s ] ).
\end{equation}


A general symmetric rank-one update has the form
\begin{equation*}
    H_{k+1} = H_k + \sigma \, v v^{\mathrm{T}},
\end{equation*}
where $v \in \mathbb{R}^n$ and $\sigma \in \{-1,1\}$. The task now is to determine $v$ and $\sigma$ so that $H_{k+1}$ satisfies the quasi-Newton equation, \cref{quasi-NewtonEquation}. By substituting into \cref{quasi-NewtonEquation}, we obtain
\begin{equation}
    H_k s_k + [\sigma \, v^{\mathrm{T}} s_k] v = y_k.
\end{equation}
Since $[\sigma \, v^{\mathrm{T}} s_k]$ is a scalar, $v$ must be a multiple of $y_k − H_k s_k$, i.e. $v = \delta (y_k − H_k s_k)$ for some $\delta \in \mathbb{R}$. By substituting this form of $v$ into , we obtain
\begin{equation}
    (y_k − H_k s_k) = \sigma \, \delta^2 \, [s^{\mathrm{T}}_k (y_k − H_k s_k)](y_k − H_k s_k),
\end{equation}
and it is clear that this equation is satisfied if (and only if) we choose the parameters $\delta$ and $\sigma$ to be
\begin{equation*}
    \sigma = \operatorname{sgn} (s^{\mathrm{T}}_k (y_k − H_k s_k)), \quad \delta = \pm \lvert s^{\mathrm{T}}_k (y_k − H_k s_k) \rvert^{-\frac{1}{2}}.
\end{equation*}


Hence the direct SR1 update for operators on tangent spaces of the manifold which approximates the action of the Riemannian Hessian $\operatorname{Hess} f(x_{k+1}) [\cdot]$ is given by 
\begin{equation}\label{RiemannianDirectSR1formula}
    \mathcal{H}^\mathrm{RSR1}_{k+1} [\cdot] = \widetilde{\mathcal{H}}^\mathrm{SR1}_k [\cdot] + \frac{(y_k - \widetilde{\mathcal{H}}^\mathrm{SR1}_k [s_k]) (y_k - \widetilde{\mathcal{H}}^\mathrm{SR1}_k [s_k])^{\flat} [\cdot] }{(y_k - \widetilde{\mathcal{H}}^\mathrm{SR1}_k [s_k])^{\flat} [s_k]}
\end{equation}
\cite[p.~18]{Huang:2013}.



It is easy to see in that even if $H^\mathrm{SR1}_k$ is positive definite, $H^\mathrm{SR1}_{k+1}$ may not have the same property (this holds also for $B^\mathrm{SR1}_k$ and $B^\mathrm{SR1}_{k+1}$). If and only if $(y_k - H^\mathrm{SR1}_k s_k)^{\mathrm{T}} s_k > 0$, the SR1 update retains positive definiteness. However, this condition is difficult to guarantee. This means that $H^\mathrm{SR1}_{k+1}$ or $B^\mathrm{SR1}_{k+1}$ may no longer be invertible. Moreover, $d_{k+1} = -{H^\mathrm{SR1}_{k+1}}^{-1} \nabla f(x_{k+1}) = -B^\mathrm{SR1}_{k+1} \nabla f(x_{k+1})$ is not necessarily a descent direction. \\
The main drawback of the SR1 update formula is that the denominator in \cref{directSR1formula} or   can vanish. In fact, even when the objective function $f$ is convex and quadratic, there may be steps on which there is no symmetric rank-one update that satisfies the quasi-Newton equation \cref{quasi-NewtonEquation}. This disadvantage results in serious numerical difficulties, which restrict the applications of it. Nevertheless, the SR1 update formula has the following advantages:
For the vanishing denominator in \cref{directSR1formula} we introduce a strategy to prevent a method using the SR1 update from breaking down. It has been observed in practice that it performs well simply by skipping the update if the denominator is small. More specifically, the update \cref{directSR1formula} is applied only if 
\begin{equation}
    \lvert (y_k - H^\mathrm{SR1}_k s_k)^{\mathrm{T}} s_k \lvert \; \geq \; r \; \lVert s_k \rVert \lVert y_k - H^\mathrm{SR1}_k s_k \rVert 
\end{equation}
holds, where $r \in (0, 1)$ is a small number, e.g. $r = 10^{−8}$. Most implementations of the SR1 update use a skipping rule of this kind. The condition $(y_k - H^\mathrm{SR1}_k s_k)^{\mathrm{T}} s_k \approx 0$ occurs infrequently, since it requires certain vectors to be aligned in a specific way. When it occurs, skipping the update appears to have no negative effects on the iteration, since the skipping condition implies that $s^{\mathrm{T}}_k \tilde{G} s_k \approx s^{\mathrm{T}}_k H^\mathrm{SR1}_k s_k$, where $\tilde{G}$ is the average Hessian over the last step, meaning that the curvature of $H^\mathrm{SR1}_k$ along $s_k$ is already correct. \\



We see immediately that \cref{RiemannianDirectSR1formula} creates a self-adjoint operator if $\mathcal{H}^\mathrm{SR1}_k$ is self-adjoint. 

There is also the possibility that this method can break down if the numerator in \cref{RiemannianDirectSR1formula} is equal to zero. In order to overcome this, can be transferred to the Riemannian setup. This means that \cref{RiemannianDirectSR1formula} is only executed if
\begin{equation}\label{RiemannianSafeguard}
    \lvert g_{x_{k+1}}(y_k - \widetilde{\mathcal{H}}^\mathrm{SR1}_k [s_k], s_k) \lvert \; \geq \; r \; \lVert s_k \rVert_{x_{k+1}} \lVert y_k - \widetilde{\mathcal{H}}^\mathrm{SR1}_k [s_k] \rVert_{x_{k+1}} 
\end{equation}
holds, where $g_{x_{k+1}}(\cdot, \cdot) \colon \; \tangent{x_{k+1}} \times \tangent{x_{k+1}} \to \mathbb{R}$ is the inner product on $\tangent{x_{k+1}}$ (see \cite[p.~6]{BergmannHerzogLouzeiroSilvaTenbrinckVidalNunez:2020:1}) and $ \lVert \cdot \rVert_{x_{k+1}} = \sqrt{g_{x_{k+1}}(\cdot, \cdot)}$ denotes the resulting norm on $\tangent{x_{k+1}}$. The factor $r$ is again in the interval $(0,1)$. \\



\begin{algorithm}[H]
    \caption{Riemannian Trust-Region Symmetric Rank-One Method}\label{RTR-SR1Method}
    \begin{algorithmic}[1]
        \State Riemannian manifold $(\mathcal{M}, g)$; isometric vector transport $\vectorTransportSymbol^S$ with $\retractionSymbol$ as associated retraction; continuously differentiable real-valued function $f$ on $\mathcal{M}$, bounded below; initial iterate $x_0 \in \mathcal{M}$; initial self-adjoint operator $\mathcal{H}^\mathrm{SR1}_0 \colon \; \tangent{x_0} \to \tangent{x_0}$; initial trust-region radius $\Delta_0 > 0$; safeguard constant $\nu \in (0,1)$; rejection boundary $\rho^{\prime} \in (0, 0.1)$; trust-region reduction factor $\tau_1 \in (0,1)$; trust-region magnification factor $\tau_2 > 1$; convergence tolerance $\varepsilon > 0$. Set $k = 0$.
        \While{$\lVert \nabla f(x_k) \rVert > \varepsilon$}
            \State Obtain $s_k$ (approximately) solving \cref{Riemanniantrsubproblem} using $\mathcal{H}^\mathrm{SR1}_k$.
            \State Set $\widetilde{x}_{k+1} = \retract{x_k}(s_k)$ and ${T^{S}_{x_k, s_k}}^{-1} ( \operatorname{grad}f(\widetilde{x}_{k+1}) ) - \operatorname{grad}f(x_k) \in \tangent{x_k}$.
            \If{\cref{RiemannianSafeguard} holds}
                \State Compute $\widetilde{\mathcal{H}}^\mathrm{SR1}_{k+1} \colon \tangent{x_{k}} \to \tangent{x_{k}}$ by means of \cref{RiemannianDirectSR1formula}.
			\Else 
				\State Set $\widetilde{\mathcal{H}}^\mathrm{SR1}_{k+1} = \mathcal{H}^\mathrm{SR1}_k$.
            \EndIf 
            \State Compute $\rho_k = \frac{f(x_k) - f(\widetilde{x}_{k+1})}{m_k(0) - m_k(s_k)}$.
            \If{$\rho_k > \rho^{\prime}$}
                \State Set $x_{k+1} = \widetilde{x}_{k+1}$ and $\mathcal{H}^\mathrm{SR1}_{k+1} = T^{S}_{x_k, s_k} \circ \widetilde{\mathcal{H}}^\mathrm{SR1}_{k+1} \circ  {T^{S}_{x_k, s_k}}^{-1} \colon \tangent{x_{k+1}} \to \tangent{x_{k+1}}$.
			\Else 
				\State Set $x_{k+1} = x_k$ and $\mathcal{H}^\mathrm{SR1}_{k+1} = \widetilde{\mathcal{H}}^\mathrm{SR1}_{k+1} \colon \tangent{x_{k}} \to \tangent{x_{k}}$.
            \EndIf 
            \If{$\rho_k > 0.75$}
                \If{$\lVert s_k \rVert \geq 0.8 \ \Delta_k$}
                    \State Set $\Delta_{k+1} = \tau_2 \ \Delta_k$.
                \Else 
                    \State Set $\Delta_{k+1} = \Delta_k$.
                \EndIf 
			\Else 
                \If{$\rho_k < 0.1$}
                    \State Set $\Delta_{k+1} = \tau_1 \ \Delta_k$.
                \Else 
                    \State Set $\Delta_{k+1} = \Delta_k$.
                \EndIf 
            \EndIf 
            \State Set $k = k+1$.
        \EndWhile
        \State \textbf{Return} $x_k$.
    \end{algorithmic}
\end{algorithm}

Moreover, we assume throughout that $f \in C^2$.

We let $\Omega$ denote the sublevel set of $x_0$, i.e.,
\begin{equation*}
    \Omega = \{ x \in \mathcal{M} \colon \ f(x) \leq f(x_0) \}.
\end{equation*}

In some results, we will assume for the retraction $\retractionSymbol$ that there exists $\mu > 0$ and $\delta_{\mu} > 0$ such that
\begin{equation}\label{RetractionAssumption}
    \lVert \xi_x \rVert_x \geq \mu \ \operatorname{dist}(x, \retract{x}(\xi_x)) \text{ for all } x \in \Omega, \text{ for all } \xi_x \in \tangent{x}, \ \lVert \xi_x \rVert_x \leq \delta_{\mu}.
\end{equation}
Such a condition is instrumental in the global convergence analysis of Riemannian trust-region schemes.

We will often require that the trust-region subproblem is solved accurately enough that, for positive constants $\sigma_1$ and $\sigma_2$,
\begin{equation}\label{RiemannianAccuracy1}
    m_k(0_{x_k}) - m_k(s_k) \geq \sigma_1 \ \lVert \operatorname{grad} f(x_k) \rVert_{x_k} \ \min \{ \Delta_k, \sigma_2 \ \frac{\lVert \operatorname{grad} f(x_k) \rVert_{x_k}}{\lVert \mathcal{H}_k \rVert} \}
\end{equation} 
and that
\begin{equation}\label{RiemannianAccuracy2}
    \mathcal{H}_k [s_k] = - \operatorname{grad} f(x_k) + \delta_k \text{ with } \delta_k \in \tangent{x_k}, \ \lVert \delta_k \rVert_{x_k} \leq \lVert \operatorname{grad} f(x_k) \rVert^{1 + \theta}_{x_k}, \text{ whenever } \lVert s_k \rVert_{x_k} < 0.8 \ \Delta_k,
\end{equation}
where $\theta > 0$ is a constant. These conditions are generalizations of \cref{accuracy1} and \cref{accuracy2}. The condition \cref{RiemannianAccuracy2} remains weaker than condition \cref{accuracy2}. The purpose of introducing $\delta_k$ in \cref{RiemannianAccuracy2} is to encompass stopping criteria such as \cite[(7.10)]{AbsilMahonySepulchre:2008} that do not require the computation of an exact solution of the trust-region subproblem. We point out in particular that \cref{RiemannianAccuracy1} and \cref{RiemannianAccuracy2} hold if the approximate solution of the trust-region subproblem is obtained from the truncated CG method, described in \cite[§~7.3.2]{AbsilMahonySepulchre:2008} in the Riemannian context.


\begin{assumption}[{\cite[Assumption~3.1]{HuangAbsilGallivan:2014}}]\label{RiemannianAssumptionsGlobalConvergence} 
    The sequence of linear operators $\{ \mathcal{H}^{\mathrm{SR1}}_k \}_k$ is bounded by a constant $M$ such that $\lVert \mathcal{H}^{\mathrm{SR1}}_k \rVert \leq M$ for all $k$.
\end{assumption}

\begin{theorem}[{\cite[Theorem~3.1]{HuangAbsilGallivan:2014}}] \label{RiemannianGlobalConvergence}
    \begin{enumerate}
        \item If $f$ is bounded below on the sublevel set $\Omega$, \cref{RiemannianAssumptionsGlobalConvergence} holds, condition \cref{RiemannianAccuracy1} holds, and \cref{RetractionAssumption} is satisfied then $\lim_{k \rightarrow \infty} \operatorname{grad} f(x_k) = 0$.
        \item If $\mathcal{M}$ is compact, Assumption 3.1 holds, and \cref{RiemannianAccuracy1} holds then $\lim_{k \rightarrow \infty} \operatorname{grad} f(x_k) = 0$, $\{ x_k \}_k$ has at least one limit point, and every limit point of $\{ x_k \}_k$ is a stationary point of $f$.
        \item If the sublevel set $\Omega$ is compact, $f$ has a unique stationary point $x^*$ in $\Omega$, Assumption 3.1 holds, condition \cref{RiemannianAccuracy1} holds, and \cref{RetractionAssumption} is satisfied then $\{ x_k \}_k$ converges to $x^*$.
    \end{enumerate}
\end{theorem}




\begin{assumption}[{\cite[Assumptions~3.2+3.3+3.4+3.5+3.6]{HuangAbsilGallivan:2014}}]\label{RiemannianAssumptionsLocalConvergence} \ \\[-1.5\baselineskip]
    \begin{enumerate}
        \item We assume that $\{ x_k \}_k$ converges to a point $x^*$.
        \item We let $\mathcal{U}_{trn}$ be a totally retractive neighborhood of $x^*$. This means that there is $\delta_{trn} > 0$ such that, for each $y \in \mathcal{U}_{trn}$, we have that $\retract{y}(\mathcal{B}(0_y, \delta_{trn})) \supseteq \mathcal{U}_{trn}$ and $\retract{y}(\cdot)$ is a diffeomorphism on $\mathcal{B}(0_y, \delta_{trn})$, where $\mathcal{B}(0_y, \delta_{trn})$ denotes the ball of radius $\delta_{trn}$ in $\tangent{y}$ centered at the origin $0_y$. We assume without loss of generality that $\{ x_k \}_k \subset \mathcal{U}_{trn}$.
        \item The point $x^*$ is a nondegenerate local minimizer of $f$. In other words, $\operatorname{grad} f(x^*) = 0$ and $\operatorname{Hess} f(x^*)$ is positive definite.
        \item There exists a constant $c$ such that for all $x, y \in \mathcal{U}_{trn}$, \begin{equation*} \lVert \operatorname{Hess} f(y) - \mathrm{T}^{S}_{x, \eta_x} \circ \operatorname{Hess} f(x) \circ {\mathrm{T}^{S}_{x, \eta_x}}^{-1} \rVert \leq c \ \operatorname{dist}(x,y), \end{equation*} where $\eta_x = {\retractionSymbol_{x}}^{-1}(y)$.
        \item There exists a constant $c_0$ such that for all $x, y \in \mathcal{U}_{trn}$, all $\xi_x \in \tangent{x}$ with $\retract{x}(\xi_x) \in \mathcal{U}_{trn}$, and all $\xi_y \in \tangent{y}$ with $\retract{y}(\xi_y) \in \mathcal{U}_{trn}$, it holds that \begin{equation*} \lVert \operatorname{Hess} \hat{f}_y(\xi_y) - \mathrm{T}^{S}_{x, \eta_x} \circ \operatorname{Hess} \hat{f}_x(\xi_x) \circ {\mathrm{T}^{S}_{x, \eta_x}}^{-1} \rVert \leq c_0 \ (\lVert \xi_x \rVert_x + \lVert \xi_y \rVert_y + \lVert \eta_x \rVert_x), \end{equation*} where $\eta_x = {\retractionSymbol_{x}}^{-1}(y)$, $\hat{f}_x(\cdot) = f \circ \retract{x}(\cdot)$ and $\hat{f}_y(\cdot) = f \circ \retract{y}(\cdot)$.
        \item For each iteration \cref{RiemannianSafeguard} holds.
        \item There exists $N$ such that, for all $k \geq N$ and all $t \in [0, 1]$, it holds that $\retract{x_k}(t s_k) \in \mathcal{U}_{trn}$.
    \end{enumerate}
\end{assumption}

\begin{theorem}[{\cite[Theorem~2.7.]{HuangAbsilGallivan:2014}}] \label{RiemannianLocalConvergence}
    If \cref{RiemannianAssumptionsGlobalConvergence} and \cref{RiemannianAssumptionsLocalConvergence} hold and the subproblem is solved accurately enough for \cref{RiemannianAccuracy1} and \cref{RiemannianAccuracy2} to hold then, the sequence $\{ x_k \}_k$ generated by \cref{RTR-SR1Method} is $n + 1$-step q-superlinear (where $n$ denotes the dimension of the manifold $\mathcal{M}$); i.e.,
    \begin{equation}
        \lim_{k \rightarrow \infty} \frac{\operatorname{dist}(x_{k+n+1}, x^*)}{\operatorname{dist}(x_k, x^*)} = 0.
    \end{equation}
\end{theorem}