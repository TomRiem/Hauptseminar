\section{Riemannian Trust-Region Symmetric Rank-One Method}

% Vector transport, flat, quasi-Newton operator, SR1 Update (unique), safeguard, algorithm, convergence


A general symmetric rank-one update has the form
\begin{equation*}
    H_{k+1} = H_k + \sigma \, v v^{\mathrm{T}},
\end{equation*}
where $v \in \mathbb{R}^n$ and $\sigma \in \{-1,1\}$. The task now is to determine $v$ and $\sigma$ so that $H_{k+1}$ satisfies the quasi-Newton equation, \cref{quasi-NewtonEquation}. By substituting into \cref{quasi-NewtonEquation}, we obtain
\begin{equation}
    H_k s_k + [\sigma \, v^{\mathrm{T}} s_k] v = y_k.
\end{equation}
Since $[\sigma \, v^{\mathrm{T}} s_k]$ is a scalar, $v$ must be a multiple of $y_k − H_k s_k$, i.e. $v = \delta (y_k − H_k s_k)$ for some $\delta \in \mathbb{R}$. By substituting this form of $v$ into , we obtain
\begin{equation}
    (y_k − H_k s_k) = \sigma \, \delta^2 \, [s^{\mathrm{T}}_k (y_k − H_k s_k)](y_k − H_k s_k),
\end{equation}
and it is clear that this equation is satisfied if (and only if) we choose the parameters $\delta$ and $\sigma$ to be
\begin{equation*}
    \sigma = \operatorname{sgn} (s^{\mathrm{T}}_k (y_k − H_k s_k)), \quad \delta = \pm \lvert s^{\mathrm{T}}_k (y_k − H_k s_k) \rvert^{-\frac{1}{2}}.
\end{equation*}


Hence the direct SR1 update for operators on tangent spaces of the manifold which approximates the action of the Riemannian Hessian $\operatorname{Hess} f(x_{k+1}) [\cdot]$ is given by 
\begin{equation}\label{RiemannianDirectSR1formula}
    \mathcal{H}^\mathrm{RSR1}_{k+1} [\cdot] = \widetilde{\mathcal{H}}^\mathrm{SR1}_k [\cdot] + \frac{(y_k - \widetilde{\mathcal{H}}^\mathrm{SR1}_k [s_k]) (y_k - \widetilde{\mathcal{H}}^\mathrm{SR1}_k [s_k])^{\flat} [\cdot] }{(y_k - \widetilde{\mathcal{H}}^\mathrm{SR1}_k [s_k])^{\flat} [s_k]}
\end{equation}
\cite[p.~18]{Huang:2013}.



It is easy to see in that even if $H^\mathrm{SR1}_k$ is positive definite, $H^\mathrm{SR1}_{k+1}$ may not have the same property (this holds also for $B^\mathrm{SR1}_k$ and $B^\mathrm{SR1}_{k+1}$). If and only if $(y_k - H^\mathrm{SR1}_k s_k)^{\mathrm{T}} s_k > 0$, the SR1 update retains positive definiteness. However, this condition is difficult to guarantee. This means that $H^\mathrm{SR1}_{k+1}$ or $B^\mathrm{SR1}_{k+1}$ may no longer be invertible. Moreover, $d_{k+1} = -{H^\mathrm{SR1}_{k+1}}^{-1} \nabla f(x_{k+1}) = -B^\mathrm{SR1}_{k+1} \nabla f(x_{k+1})$ is not necessarily a descent direction. \\
The main drawback of the SR1 update formula is that the denominator in \cref{directSR1formula} or   can vanish. In fact, even when the objective function $f$ is convex and quadratic, there may be steps on which there is no symmetric rank-one update that satisfies the quasi-Newton equation \cref{quasi-NewtonEquation}. This disadvantage results in serious numerical difficulties, which restrict the applications of it. Nevertheless, the SR1 update formula has the following advantages:
For the vanishing denominator in \cref{directSR1formula} we introduce a strategy to prevent a method using the SR1 update from breaking down. It has been observed in practice that it performs well simply by skipping the update if the denominator is small. More specifically, the update \cref{directSR1formula} is applied only if 
\begin{equation}
    \lvert (y_k - H^\mathrm{SR1}_k s_k)^{\mathrm{T}} s_k \lvert \; \geq \; r \; \lVert s_k \rVert \lVert y_k - H^\mathrm{SR1}_k s_k \rVert 
\end{equation}
holds, where $r \in (0, 1)$ is a small number, e.g. $r = 10^{−8}$. Most implementations of the SR1 update use a skipping rule of this kind. The condition $(y_k - H^\mathrm{SR1}_k s_k)^{\mathrm{T}} s_k \approx 0$ occurs infrequently, since it requires certain vectors to be aligned in a specific way. When it occurs, skipping the update appears to have no negative effects on the iteration, since the skipping condition implies that $s^{\mathrm{T}}_k \tilde{G} s_k \approx s^{\mathrm{T}}_k H^\mathrm{SR1}_k s_k$, where $\tilde{G}$ is the average Hessian over the last step, meaning that the curvature of $H^\mathrm{SR1}_k$ along $s_k$ is already correct. \\



We see immediately that \cref{RiemannianDirectSR1formula} creates a self-adjoint operator if $\mathcal{H}^\mathrm{SR1}_k$ is self-adjoint. 

There is also the possibility that this method can break down if the numerator in \cref{RiemannianDirectSR1formula} is equal to zero. In order to overcome this, can be transferred to the Riemannian setup. This means that \cref{RiemannianDirectSR1formula} is only executed if
\begin{equation}\label{RiemannianSafeguard}
    \lvert g_{x_{k+1}}(y_k - \widetilde{\mathcal{H}}^\mathrm{SR1}_k [s_k], s_k) \lvert \; \geq \; r \; \lVert s_k \rVert_{x_{k+1}} \lVert y_k - \widetilde{\mathcal{H}}^\mathrm{SR1}_k [s_k] \rVert_{x_{k+1}} 
\end{equation}
holds, where $g_{x_{k+1}}(\cdot, \cdot) \colon \; \tangent{x_{k+1}} \times \tangent{x_{k+1}} \to \mathbb{R}$ is the inner product on $\tangent{x_{k+1}}$ (see \cite[p.~6]{BergmannHerzogLouzeiroSilvaTenbrinckVidalNunez:2020:1}) and $ \lVert \cdot \rVert_{x_{k+1}} = \sqrt{g_{x_{k+1}}(\cdot, \cdot)}$ denotes the resulting norm on $\tangent{x_{k+1}}$. The factor $r$ is again in the interval $(0,1)$. \\



\begin{algorithm}[H]
    \caption{Riemannian Trust-Region Symmetric Rank-One Method}\label{RTR-SR1Method}
    \begin{algorithmic}[1]
        \State Riemannian manifold $(\mathcal{M}, g)$; isometric vector transport $\vectorTransportSymbol^S$ with $\retractionSymbol$ as associated retraction; continuously differentiable real-valued function $f$ on $\mathcal{M}$, bounded below; initial iterate $x_0 \in \mathcal{M}$; initial self-adjoint operator $\mathcal{H}^\mathrm{SR1}_0 \colon \; \tangent{x_0} \to \tangent{x_0}$; initial trust-region radius $\Delta_0 > 0$; safeguard constant $\nu \in (0,1)$; rejection boundary $\rho^{\prime} \in (0, 0.1)$; trust-region reduction factor $\tau_1 \in (0,1)$; trust-region magnification factor $\tau_2 > 1$; convergence tolerance $\varepsilon > 0$. Set $k = 0$.
        \While{$\lVert \nabla f(x_k) \rVert > \varepsilon$}
            \State Obtain $s_k$ (approximately) solving \cref{Riemanniantrsubproblem} using $\mathcal{H}^\mathrm{SR1}_k$.
            \State Set $\widetilde{x}_{k+1} = \retract{x_k}(s_k)$ and ${T^{S}_{x_k, s_k}}^{-1} ( \operatorname{grad}f(\widetilde{x}_{k+1}) ) - \operatorname{grad}f(x_k) \in \tangent{x_k}$.
            \If{\cref{RiemannianSafeguard} holds}
                \State Compute $\widetilde{\mathcal{H}}^\mathrm{SR1}_{k+1} \colon \tangent{x_{k}} \to \tangent{x_{k}}$ by means of \cref{RiemannianDirectSR1formula}.
			\Else 
				\State Set $\widetilde{\mathcal{H}}^\mathrm{SR1}_{k+1} = \mathcal{H}^\mathrm{SR1}_k$.
            \EndIf 
            \State Compute $\rho_k = \frac{f(x_k) - f(\widetilde{x}_{k+1})}{m_k(0) - m_k(s_k)}$.
            \If{$\rho_k > \rho^{\prime}$}
                \State Set $x_{k+1} = \widetilde{x}_{k+1}$ and $\mathcal{H}^\mathrm{SR1}_{k+1} = T^{S}_{x_k, s_k} \circ \widetilde{\mathcal{H}}^\mathrm{SR1}_{k+1} \circ  {T^{S}_{x_k, s_k}}^{-1} \colon \tangent{x_{k+1}} \to \tangent{x_{k+1}}$.
			\Else 
				\State Set $x_{k+1} = x_k$ and $\mathcal{H}^\mathrm{SR1}_{k+1} = \widetilde{\mathcal{H}}^\mathrm{SR1}_{k+1} \colon \tangent{x_{k}} \to \tangent{x_{k}}$.
            \EndIf 
            \If{$\rho_k > 0.75$}
                \If{$\lVert s_k \rVert \geq 0.8 \ \Delta_k$}
                    \State Set $\Delta_{k+1} = \tau_2 \ \Delta_k$.
                \Else 
                    \State Set $\Delta_{k+1} = \Delta_k$.
                \EndIf 
			\Else 
                \If{$\rho_k < 0.1$}
                    \State Set $\Delta_{k+1} = \tau_1 \ \Delta_k$.
                \Else 
                    \State Set $\Delta_{k+1} = \Delta_k$.
                \EndIf 
            \EndIf 
            \State Set $k = k+1$.
        \EndWhile
        \State \textbf{Return} $x_k$.
    \end{algorithmic}
\end{algorithm}

Moreover, we assume throughout that $f \in C^2$.

We let $\Omega$ denote the sublevel set of $x_0$, i.e.,
\begin{equation*}
    \Omega = \{ x \in \mathcal{M} \colon \ f(x) \leq f(x_0) \}.
\end{equation*}

In some results, we will assume for the retraction $\retractionSymbol$ that there exists $\mu > 0$ and $\delta_{\mu} > 0$ such that
\begin{equation}\label{RetractionAssumption}
    \lVert \xi_x \rVert_x \geq \mu \ \operatorname{dist}(x, \retract{x}(\xi_x)) \text{ for all } x \in \Omega, \text{ for all } \xi_x \in \tangent{x}, \ \lVert \xi_x \rVert_x \leq \delta_{\mu}.
\end{equation}
Such a condition is instrumental in the global convergence analysis of Riemannian trust-region schemes.

We will often require that the trust-region subproblem is solved accurately enough that, for positive constants $\sigma_1$ and $\sigma_2$,
\begin{equation}\label{RiemannianAccuracy1}
    m_k(0_{x_k}) - m_k(s_k) \geq \sigma_1 \ \lVert \operatorname{grad} f(x_k) \rVert_{x_k} \ \min \{ \Delta_k, \sigma_2 \ \frac{\lVert \operatorname{grad} f(x_k) \rVert_{x_k}}{\lVert \mathcal{H}_k \rVert} \}
\end{equation} 
and that
\begin{equation}\label{RiemannianAccuracy2}
    \mathcal{H}_k [s_k] = - \operatorname{grad} f(x_k) + \delta_k \text{ with } \delta_k \in \tangent{x_k}, \ \lVert \delta_k \rVert_{x_k} \leq \lVert \operatorname{grad} f(x_k) \rVert^{1 + \theta}_{x_k}, \text{ whenever } \lVert s_k \rVert_{x_k} < 0.8 \ \Delta_k,
\end{equation}
where $\theta > 0$ is a constant. These conditions are generalizations of \cref{accuracy1} and \cref{accuracy2}. The condition \cref{RiemannianAccuracy2} remains weaker than condition \cref{accuracy2}. The purpose of introducing $\delta_k$ in \cref{RiemannianAccuracy2} is to encompass stopping criteria such as \cite[(7.10)]{AbsilMahonySepulchre:2008} that do not require the computation of an exact solution of the trust-region subproblem. We point out in particular that \cref{RiemannianAccuracy1} and \cref{RiemannianAccuracy2} hold if the approximate solution of the trust-region subproblem is obtained from the truncated CG method, described in \cite[§~7.3.2]{AbsilMahonySepulchre:2008} in the Riemannian context.


\begin{assumption}[{\cite[Assumption~3.1]{HuangAbsilGallivan:2014}}]\label{RiemannianAssumptionsGlobalConvergence} 
    The sequence of linear operators $\{ \mathcal{H}^{\mathrm{SR1}}_k \}_k$ is bounded by a constant $M$ such that $\lVert \mathcal{H}^{\mathrm{SR1}}_k \rVert \leq M$ for all $k$.
\end{assumption}

\begin{theorem}[{\cite[Theorem~3.1]{HuangAbsilGallivan:2014}}] \label{RiemannianGlobalConvergence}
    \begin{enumerate}
        \item If $f$ is bounded below on the sublevel set $\Omega$, \cref{RiemannianAssumptionsGlobalConvergence} holds, condition \cref{RiemannianAccuracy1} holds, and \cref{RetractionAssumption} is satisfied then $\lim_{k \rightarrow \infty} \operatorname{grad} f(x_k) = 0$.
        \item If $\mathcal{M}$ is compact, Assumption 3.1 holds, and \cref{RiemannianAccuracy1} holds then $\lim_{k \rightarrow \infty} \operatorname{grad} f(x_k) = 0$, $\{ x_k \}_k$ has at least one limit point, and every limit point of $\{ x_k \}_k$ is a stationary point of $f$.
        \item If the sublevel set $\Omega$ is compact, $f$ has a unique stationary point $x^*$ in $\Omega$, Assumption 3.1 holds, condition \cref{RiemannianAccuracy1} holds, and \cref{RetractionAssumption} is satisfied then $\{ x_k \}_k$ converges to $x^*$.
    \end{enumerate}
\end{theorem}




\begin{assumption}[{\cite[Assumptions~3.2+3.3+3.4+3.5+3.6]{HuangAbsilGallivan:2014}}]\label{RiemannianAssumptionsLocalConvergence} \ \\[-1.5\baselineskip]
    \begin{enumerate}
        \item We assume that $\{ x_k \}_k$ converges to a point $x^*$.
        \item We let $\mathcal{U}_{trn}$ be a totally retractive neighborhood of $x^*$. This means that there is $\delta_{trn} > 0$ such that, for each $y \in \mathcal{U}_{trn}$, we have that $\retract{y}(\mathcal{B}(0_y, \delta_{trn})) \supseteq \mathcal{U}_{trn}$ and $\retract{y}(\cdot)$ is a diffeomorphism on $\mathcal{B}(0_y, \delta_{trn})$, where $\mathcal{B}(0_y, \delta_{trn})$ denotes the ball of radius $\delta_{trn}$ in $\tangent{y}$ centered at the origin $0_y$. We assume without loss of generality that $\{ x_k \}_k \subset \mathcal{U}_{trn}$.
        \item The point $x^*$ is a nondegenerate local minimizer of $f$. In other words, $\operatorname{grad} f(x^*) = 0$ and $\operatorname{Hess} f(x^*)$ is positive definite.
        \item There exists a constant $c$ such that for all $x, y \in \mathcal{U}_{trn}$, \begin{equation*} \lVert \operatorname{Hess} f(y) - \mathrm{T}^{S}_{x, \eta_x} \circ \operatorname{Hess} f(x) \circ {\mathrm{T}^{S}_{x, \eta_x}}^{-1} \rVert \leq c \ \operatorname{dist}(x,y), \end{equation*} where $\eta_x = {\retractionSymbol_{x}}^{-1}(y)$.
        \item There exists a constant $c_0$ such that for all $x, y \in \mathcal{U}_{trn}$, all $\xi_x \in \tangent{x}$ with $\retract{x}(\xi_x) \in \mathcal{U}_{trn}$, and all $\xi_y \in \tangent{y}$ with $\retract{y}(\xi_y) \in \mathcal{U}_{trn}$, it holds that \begin{equation*} \lVert \operatorname{Hess} \hat{f}_y(\xi_y) - \mathrm{T}^{S}_{x, \eta_x} \circ \operatorname{Hess} \hat{f}_x(\xi_x) \circ {\mathrm{T}^{S}_{x, \eta_x}}^{-1} \rVert \leq c_0 \ (\lVert \xi_x \rVert_x + \lVert \xi_y \rVert_y + \lVert \eta_x \rVert_x), \end{equation*} where $\eta_x = {\retractionSymbol_{x}}^{-1}(y)$, $\hat{f}_x(\cdot) = f \circ \retract{x}(\cdot)$ and $\hat{f}_y(\cdot) = f \circ \retract{y}(\cdot)$.
        \item For each iteration \cref{RiemannianSafeguard} holds.
        \item There exists $N$ such that, for all $k \geq N$ and all $t \in [0, 1]$, it holds that $\retract{x_k}(t s_k) \in \mathcal{U}_{trn}$.
    \end{enumerate}
\end{assumption}

\begin{theorem}[{\cite[Theorem~2.7.]{HuangAbsilGallivan:2014}}] \label{RiemannianLocalConvergence}
    Consider \cref{TR-SR1Method} and suppose that \cref{AssumptionsGlobalConvergence} and \cref{AssumptionsLocalConvergence} hold. Then the sequence $\{ x_k \}_k$ generated by \cref{TR-SR1Method} converges $n+l$-step superlinear, i.e. 
    \begin{equation}
        \lim_{k \rightarrow \infty} \frac{\lVert x_{k+n+1} - x^* \rVert}{\lVert x_k - x^* \rVert} = 0.
    \end{equation}
\end{theorem}