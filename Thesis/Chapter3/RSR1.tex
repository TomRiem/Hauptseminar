\section{Riemannian Trust-Region Symmetric Rank-One Method}

We will first consider Trust-Region methods on Riemannian manifolds in general. To do this, we must construct a quadratic model of our objective function $f$, which is defined on a Riemannian manifold $\mathcal{M}$. But this is not, in general, a Euclidean space, which makes the situation quite difficult. To avoid this problem, we choose a retraction $\retractionSymbol$, which provides a way to pull back the objective function on the manifold $\mathcal{M}$ to a objective function on the tangent space $\tangent{x_k}$ at each iterate. \\
Given a objective function $f \colon \mathcal{M} \to \mathbb{R}$ and a current iterate $x_k \in \mathcal{M}$, we use $\retract{x_k}$ to locally map the minimization problem for $f$ on $\mathcal{M}$ into a minimization problem for the pullback $\hat{f}_{x_k}$ of $f$ under $\retract{x_k}$ on the tangent space $\tangent{x_k}$,
\begin{align*}
    \hat{f}_{x_k} \colon \tangent{x_k} & \to \mathbb{R} \\
    \xi_{x_k} & \mapsto \hat{f}_{x_k} (\xi_{x_k}) = f(\retract{x_k}(\xi_{x_k})).
\end{align*}
The Riemannian metric turns the tangent space $\tangent{x_k}$ into a Euclidean space endowed with the inner product $g_{x_k}(\cdot, \cdot)$, which enables us to construct a order-2 model of the pullback $\hat{f}_{x_k}$:
\begin{align*}
    \hat{m}_k( s ) & = \hat{f}_{x_k}(0_{x_k}) + \mathrm{D} \, \hat{f}_{x_k}(0_{x_k}) [s] + \frac{1}{2} \mathrm{D} \, \hat{f}_{x_k}(0_{x_k}) [s, s] \\
    & = f(x_k) + g_{x_k}(\operatorname{grad} f(x_k), s ) + \frac{1}{2} g_{x_k}( s, \operatorname{Hess} \hat{f}_{x_k}(0_{x_k}) [s]).
\end{align*}
By \cref{Retraction} we get $\hat{f}_{x_k}(0_{x_k}) = f(\retract{x_k}(0_{x_k})) = f(x_k)$ and since the local rigidity condition \cref{LocalRigidity} holds, we have $\mathrm{D} \, \hat{f}_{x_k}(0_{x_k}) = \mathrm{D} \, f(x_k)$, hence $\operatorname{grad} \hat{f}_{x_k}(0_{x_k}) = \operatorname{grad} f(x_k)$. \\
Finally to get a model of $f$, we “push forward” the model $\hat{m}_k(s)$ through the retraction $\retract{x_k}$, i.e.
\begin{equation}\label{RiemannianModel}
    m_k = \hat{m}_k \circ \inverseRetract{x_k}
\end{equation}
with
\begin{equation}\label{RiemannianModelTangent}
    \hat{m}_k( s ) = f(x_k) + g_{x_k}(\operatorname{grad} f(x_k), s ) + \frac{1}{2} g_{x_k}( s, \operatorname{Hess} f(x_k) [s]),
\end{equation}
where the quadratic term is given by the Riemannian Hessian of the objective function $f$. \\
In general, the model \cref{RiemannianModel} (with \cref{RiemannianModelTangent}) of the objective function $f$ is of order 1 because $\operatorname{Hess} \hat{f}_{x_k}(0_{x_k}) \neq \operatorname{Hess} f(x_k)$. Since the Hessian operator of the pullback depends among other things on the chosen retraction, further conditions must be imposed on the retraction so that this equality holds (see \cite[Proposition~5.5.5]{AbsilMahonySepulchre:2008}). However, for any retraction, $\operatorname{Hess} \hat{f}_{x^*}(0_{x^*}) = \operatorname{Hess} f(x^*)$ holds, when $x^*$ is a stationary point of $f$, i.e. $\operatorname{grad} f(x^*) = 0_{x^*}$ \cite[p.~138-139]{AbsilMahonySepulchre:2008}. \\
For now we just consider a quadratic model of the pullback $\hat{f}_{x_k}$:
\begin{equation}\label{RiemannianQuadraticModel}
    \hat{m}_k( s ) = f(x_k) + g_{x_k}(\operatorname{grad} f(x_k), s) + \frac{1}{2} g_{x_k}( s, \mathcal{H}_k [s]),
\end{equation}
where $\mathcal{H}_k$ is some linear self-adjoint operator on $\tangent{x_k}$. This model \cref{RiemannianQuadraticModel} is easier to handle than the pullback $\hat{f}_{x_k}$ and its purpose is to approximate $\hat{f}_{x_k}$ within a suitable neighbourhood of the zero tangent vector $0_{x_k} \in \tangent{x_k}$ which we refer to as the trust-region. By using the norm $\lVert \cdot \rVert_{x_k}$ induced by the Riemannian metric, we can define the trust-region as the set of all points
\begin{equation}\label{RiemannianTrustRegion}
    \{ s \in \tangent{x_k} \colon \ \lVert s \rVert_{x_k} \leq \Delta_k \}
\end{equation}
where $\Delta_k > 0$ is the trust-region radius. We point out that this trust-region now consists not of all points on the manifold which have the distance less or equal $\Delta_k$ to the current iterate $x_k$, but of all tangent vectors in tangent space at the current iterate $\tangent{x_k}$ whose length is less or equal $\Delta_k$. The chosen retraction $\retractionSymbol$ defines for any iterate $x_k \in \mathcal{M}$, a one-to-one correspondence $\retract{x_k}$ between a neighborhood of $x_k$ in $\mathcal{M}$ and a neighborhood of $0_{x_k}$ in the tangent space $\tangent{x_k}$ (we remember $\retract{x_k}(0_{x_k}) = x_k$) \cite[p.~304]{AbsilBakerGallivan:2007}. But the chosen retraction $\retractionSymbol$ applied to a tangent vector $\xi_{x_k} \in \tangent{x_k}$ generally does not lead to a point $\retract{x_k}(\xi_{x_k})$ whose distance to the starting point $x_k$ is equal to the norm of the tangent vector $\xi_{x_k}$, i.e. $\operatorname{dist}(x_k, \retract{x_k}(\xi_{x_k})) \neq \lVert \xi_{x_k} \rVert_{x_k}$. \\
Next, we compute the step $s_k \in \tangent{x_k}$ as an (approximate) solution of the trust-region subproblem given by the model \cref{RiemannianQuadraticModel} and the trust-region \cref{RiemannianTrustRegion}, i.e.
\begin{equation}\label{Riemanniantrsubproblem}
    s_k = \arg \min_{\lVert s \rVert_{x_k} \leq \Delta_k} \hat{m}_k( s ) \in \tangent{x_k}.
\end{equation}
Since $\tangent{x_k}$ is a Euclidean space, it is possible to adapt classical methods in $R^n$ to compute a approximate minimizer of the trust-region subproblem \cref{Riemanniantrsubproblem} \cite[p.~304]{AbsilBakerGallivan:2007}. A possible method for this is given by \cite[Algorithm~11]{AbsilMahonySepulchre:2008}, which is a generalization of the truncated conjugate-gradient method. \\
This minimizer $s_k$ is then retracted back from $\tangent{x_k}$ to $\mathcal{M}$, i.e.
\begin{equation*}
    \widetilde{x}_{k+1} = \retract{x_k}(s_k).
\end{equation*}
This point is a candidate for the new iterate $x_{k+1}$. The decisions on accepting or rejecting the candidate $\widetilde{x}_{k+1}$ and on selecting the new trust-region radius $\Delta_k$ are based on the quotient
\begin{equation*}
    \rho_k = \frac{\hat{f}_{x_k}(0_{x_k}) - \hat{f}_{x_k}(s_k)}{\hat{m}_k(0_{x_k}) - \hat{m}_k(s_k)} = \frac{f(x_k) - f(\retract{x_k}(s_k))}{\hat{m}_k(0_{x_k}) - \hat{m}_k(s_k)} = \frac{f(x_k) - f(\widetilde{x}_{k+1})}{\hat{m}_k(0_{x_k}) - \hat{m}_k(s_k)}.
\end{equation*}
The same decision parameters are here used and the update of the trust-region radius follows the same heuristics as in the Euclidean case. \\ 
We point out that this “pullback-solve-retract” distinguishes the Riemannian Trust-Region approach from the Euclidean Trust-Region methods, which only require the “solve” part since they live in $\mathbb{R}^n$. On a manifold, using the pullback $\hat{f}_{x_k}$ of the objective function $f$ makes it possible to locally fall back to a friendly Euclidean world (the tangent space $\tangent{x_k}$) where classical techniques can be applied, and the retraction $\retract{x_k}$ brings the result back to the manifold $\mathcal{M}$. A difficulty, from an analysis perspective, is that the Riemannian Trust-Region approach does not deal with a unique objective function $f$,
but rather with a succession of different pullbacks $\hat{f}_{x_k}$ of the objetcive function \cite[p.~305]{AbsilBakerGallivan:2007}. \\

Let us now turn to the quadratic term $\mathcal{H}_k \colon \tangent{x_k} \to \tangent{x_k}$ in \cref{RiemannianQuadraticModel}. It can be shown, if $\mathcal{H}_k$ is a sufficiently good approximation of the Hessian operator $\operatorname{Hess} f(x_k)$ and under further (strong) assumptions, among others on the retraction $\retractionSymbol$, that the the sequence $\{ x_k \}_k$ from the resulting method converges q-superlinearly (see \cite[Theorem~7.4.11]{AbsilMahonySepulchre:2008}). A possible choice would be the Riemannian Hessian $\mathcal{H}_k = \operatorname{Hess} f(x_k)$ and for $\retractionSymbol$ the exponential map $\expOp$ (see \cite[p.~102]{AbsilMahonySepulchre:2008}). But as in the Euclidean case, the application of $\operatorname{Hess} f(x_k)$ can be very costly or $\operatorname{Hess} f(x_k)$ doesn't even exist and for $\expOp$ these can also be reasons why one does not use this method. Therefore, we are looking for an approximation of $\operatorname{Hess} f(x_k)$ that is easy to compute but still produces a fast rate of convergence. This leads us to genralizing the SR1 update \cref{directSR1formula} for the Riemannian setup. \\
This means we want to approximate the action of the Riemannian Hessian $\operatorname{Hess} f(x_k)$ with a linear self-adjoint operator $\mathcal{H}^\mathrm{SR1}_k \colon \tangent{x_k} \to \tangent{x_k}$, which is updated (in each iteration) with information about the curvature obtained by $s_k, \operatorname{grad} f(x_k) \in \tangent{x_k}$ and $\operatorname{grad} f(\widetilde{x}_{k+1}) \in \tangent{\widetilde{x}_{k+1}}$ to a new operator $\mathcal{H}^\mathrm{SR1}_{k+1} \colon \tangent{x_{k+1}} \to \tangent{x_{k+1}}$, which acts on the tangent space at the upcomming iterate $x_{k+1}$. To be able to work with the information from different tangent spaces, we use a so-called vector transport $\vectorTransportSymbol$ on a manifold $\mathcal{M}$, which is a smooth map 
\begin{align*}
    \vectorTransportSymbol \colon \; \tangent{} \oplus \tangent{} & \to \tangent{} \\
    (\eta_x, \xi_x) & \mapsto \vectorTransportDir{x}{\eta_x}(\xi_x)
\end{align*}      
satisfying the following properties for all $x \in \mathcal{M}$:
\begin{enumerate}
    \item (Associated retraction) $\vectorTransportDir{x}{\eta_x}(\xi_x) \in \tangent{\retract{x}(\eta_x)}$ for all $\xi_x \in \tangent{x}$;
    \item (Consistency) $\vectorTransportDir{x}{0_x}(\xi_x) = \xi_x$ for all $\xi_x \in \tangent{x}$;
    \item (Linearity) $\vectorTransportDir{x}{\eta_x}(a \xi_x + b \zeta_x) = a \vectorTransportDir{x}{\eta_x}(\xi_x) + b \vectorTransportDir{x}{\eta_x}(\zeta_x)$;
\end{enumerate}
\cite[Definition~8.1.1]{AbsilMahonySepulchre:2008}. A vector transport $\vectorTransportSymbol^S \colon \; \tangent{} \oplus \tangent{} \to \tangent{}$ with associated retraction $\retractionSymbol$ is called isometric if it satisfies for all $x \in \mathcal{M}$
\begin{equation}\label{IsometricVectorTransport}
    g_{\retract{x}(\eta_x)}(\vectorTransportDir{x}{\eta_x}(\xi_x)[S], \vectorTransportDir{x}{\eta_x}(\zeta_x)[S]) = g_x (\xi_x, \zeta_x)
\end{equation}
for all $\eta_x, \xi_x, \zeta_x \in \tangent{x}$ \cite[p.~10]{Huang:2013}. We use $\vectorTransportSymbol^S$ to denote an isometric vector transport. \\
Thus, we are now able to summarize the information for the update in one tangent space. Since, as in the Euclidean case, we want to update $\mathcal{H}^\mathrm{SR1}_k$ before deciding whether or not to accept the candidate $\widetilde{x}_{k+1}$, we use the tangent space at the current iterate $\tangent{x_k}$. For this we define $y_k = {\mathrm{T}^{S}_{x_k, s_k}}^{-1} ( \operatorname{grad}f(\widetilde{x}_{k+1}) ) - \operatorname{grad}f(x_k) \in \tangent{x_k}$, where the associated retraction of $\vectorTransportSymbol^S$ is our chosen retraction $\retractionSymbol$. \\
To be able to create rank-one operators, we introduce the musical isomorphism $\flat \colon \; \tangent{x} \ni \eta_{x} \mapsto \eta^{\flat}_{x} \in \cotangent{x}$ (see \cite[p.~6]{BergmannHerzogLouzeiroSilvaTenbrinckVidalNunez:2020:1}). Put simply, it means: $\eta^{\flat}_{x} \in \cotangent{x}$ represents the flat of $\eta_{x} \in \tangent{x}$, i.e., $\eta^{\flat}_{x} \colon \; \tangent{x} \to \mathbb{R}, \; \xi_{x} \mapsto \eta^{\flat}_{x}[\xi_{x}] = g_{x} (\eta_{x}, \xi_{x})$. This generalizes the notion of the transpose from the Euclidean case. It can be shown that $\eta_{x} \eta^{\flat}_{x} \tangent{x} \to \tangent{x}$ is a linear self-adjoint positive definite rank-one operator. \\
With $s_k \in \tangent{x_k}$, which generalizes the connection between the current iterate $x_k$ and the candidate $\widetilde{x}_{k+1}$, $y_k \in \tangent{x_k}$, which generalizes the difference of the gradients at $x_k$ and $\widetilde{x}_{k+1}$, and by introducing the notion of the flat $\flat$, we can now define a self-adjoint rank-one, also short SR1, update for operators on the tangent space at the current iterate $\tangent{x_k}$:
\begin{equation}\label{RiemannianDirectSR1formula}
    \widetilde{\mathcal{H}}^{SR1}_{k+1} [\cdot] = \mathcal{H}^\mathrm{SR1}_k [\cdot] + \frac{(y_k - \mathcal{H}^\mathrm{SR1}_k [s_k]) (y_k - \mathcal{H}^\mathrm{SR1}_k [s_k])^{\flat} [\cdot] }{(y_k - \mathcal{H}^\mathrm{SR1}_k [s_k])^{\flat} [s_k]}.
\end{equation}
We see immediately that \cref{RiemannianDirectSR1formula} creates a self-adjoint operator if $\mathcal{H}^\mathrm{SR1}_k$ is self-adjoint. But even if $\mathcal{H}^\mathrm{SR1}_k$ is positive definite, $\widetilde{\mathcal{H}}^{SR1}_{k+1}$ may not have the same property. If and only if $(y_k - \mathcal{H}^\mathrm{SR1}_k [s_k])^{\flat} [s_k] > 0$, the update \cref{RiemannianDirectSR1formula} retains positive definiteness. \\
As in the Euclidean case, the update \cref{RiemannianDirectSR1formula} has a close connection to Riemannian quasi-Newton methods, where it is required that the approximation of the Hessian operator $\operatorname{Hess} f(x_{k+1})$ generated by the corresponding update satisfies some kind of Riemann quasi-Newton equation (for more details see \cite[Chapter~2]{Huang:2013}). For \cref{RiemannianDirectSR1formula}, it can be shown that 
\begin{equation*}
    \widetilde{\mathcal{H}}^{SR1}_{k+1} [s_k] = y_k 
\end{equation*}
holds, which generalizes \cref{quasi-NewtonEquation} in this context. \\
As in the Euclidean case, the update \cref{RiemannianDirectSR1formula} has the disadvantage that the numerator of the self-adjoint rank-one operator, which is added to the current approximation $\mathcal{H}^\mathrm{SR1}_k$, can vanish. This can also lead to numerical difficulties or even to the breakdown of the method. To avoid this, we generalize the strategy \cref{safeguard} we know from the Euclidean case: 
\begin{equation}\label{RiemannianSafeguard}
    \lvert g_{x_k}(y_k - \mathcal{H}^{SR1}_k[s_k], \ s_k) \rvert \geq r \; \lVert y_k - \mathcal{H}^{SR1}_k[s_k] \rVert_{x_k} \ \lVert s_k \rVert_{x_k},
\end{equation}
where $r \in (0, 1)$ is again a small number. \\
Right now have the operator $\widetilde{\mathcal{H}}^{SR1}_{k+1} \colon \tangent{x_k} \to \tangent{x_k}$, which is defined on the tangent space at the current iterate $x_k$, but for the next iteration we need an operator $\mathcal{H}^{SR1}_{k+1} \colon \tangent{x_{k+1}} \to \tangent{x_{k+1}}$ on the tangent space at the coming iterate $x_{k+1}$, but $x_{k+1}$ depends on whether we accept or reject the candidate $\widetilde{x}_{k+1}$. Therefore, we transport the operator $\widetilde{\mathcal{H}}^{SR1}_{k+1}$ into the tangent space $\tangent{x_{k+1}} = \tangent{\widetilde{x}_{k+1}}$ if we accept $\widetilde{x}_{k+1}$ as the next iterate $x_{k+1}$, i.e. if $x_{k+1} = \widetilde{x}_{k+1}$ we set $\mathcal{H}^{SR1}_{k+1} = \mathrm{T}^{S}_{x_k, s_k} \circ \widetilde{\mathcal{H}}^{SR1}_{k+1} \circ {\mathrm{T}^{S}_{x_k, s_k}}^{-1}$. If we do not accept the candidate $\widetilde{x}_{k+1}$, i.e. $x_{k+1} = x_k$, then we set $\mathcal{H}^{SR1}_{k+1} = \widetilde{\mathcal{H}}^{SR1}_{k+1}$, since $\widetilde{\mathcal{H}}^{SR1}_{k+1}$ is already an operator on the tangent space at the current iterate $x_k$. \\

\begin{algorithm}[H]
    \caption{Riemannian Trust-Region Symmetric Rank-One Method}\label{RTR-SR1Method}
    \begin{algorithmic}[1]
        \State Riemannian manifold $(\mathcal{M}, g)$; isometric vector transport $\vectorTransportSymbol^S$ with $\retractionSymbol$ as associated retraction; continuously differentiable real-valued function $f$ on $\mathcal{M}$, bounded below; initial iterate $x_0 \in \mathcal{M}$; initial self-adjoint operator $\mathcal{H}^\mathrm{SR1}_0 \colon \; \tangent{x_0} \to \tangent{x_0}$; initial trust-region radius $\Delta_0 > 0$; safeguard constant $r \in (0,1)$; rejection boundary $\rho^{\prime} \in (0, 0.1)$; trust-region reduction factor $\tau_1 \in (0,1)$; trust-region magnification factor $\tau_2 > 1$; convergence tolerance $\varepsilon > 0$. Set $k = 0$.
        \While{$\lVert \nabla f(x_k) \rVert > \varepsilon$}
            \State Obtain $s_k$ (approximately) solving \cref{Riemanniantrsubproblem} using $\mathcal{H}^\mathrm{SR1}_k$.
            \State Set $\widetilde{x}_{k+1} = \retract{x_k}(s_k)$ and ${T^{S}_{x_k, s_k}}^{-1} ( \operatorname{grad}f(\widetilde{x}_{k+1}) ) - \operatorname{grad}f(x_k) \in \tangent{x_k}$.
            \If{\cref{RiemannianSafeguard} holds}
                \State Compute $\widetilde{\mathcal{H}}^\mathrm{SR1}_{k+1} \colon \tangent{x_{k}} \to \tangent{x_{k}}$ by means of \cref{RiemannianDirectSR1formula}.
			\Else 
				\State Set $\widetilde{\mathcal{H}}^\mathrm{SR1}_{k+1} = \mathcal{H}^\mathrm{SR1}_k$.
            \EndIf 
            \State Compute $\rho_k = \frac{f(x_k) - f(\widetilde{x}_{k+1})}{m_k(0) - m_k(s_k)}$.
            \If{$\rho_k > \rho^{\prime}$}
                \State Set $x_{k+1} = \widetilde{x}_{k+1}$ and $\mathcal{H}^\mathrm{SR1}_{k+1} = T^{S}_{x_k, s_k} \circ \widetilde{\mathcal{H}}^\mathrm{SR1}_{k+1} \circ  {T^{S}_{x_k, s_k}}^{-1} \colon \tangent{x_{k+1}} \to \tangent{x_{k+1}}$.
			\Else 
				\State Set $x_{k+1} = x_k$ and $\mathcal{H}^\mathrm{SR1}_{k+1} = \widetilde{\mathcal{H}}^\mathrm{SR1}_{k+1} \colon \tangent{x_{k}} \to \tangent{x_{k}}$.
            \EndIf 
            \If{$\rho_k > 0.75$}
                \If{$\lVert s_k \rVert \geq 0.8 \ \Delta_k$}
                    \State Set $\Delta_{k+1} = \tau_2 \ \Delta_k$.
                \Else 
                    \State Set $\Delta_{k+1} = \Delta_k$.
                \EndIf 
			\Else 
                \If{$\rho_k < 0.1$}
                    \State Set $\Delta_{k+1} = \tau_1 \ \Delta_k$.
                \Else 
                    \State Set $\Delta_{k+1} = \Delta_k$.
                \EndIf 
            \EndIf 
            \State Set $k = k+1$.
        \EndWhile
        \State \textbf{Return} $x_k$.
    \end{algorithmic}
\end{algorithm}

We summarize the most important results of the convergence analysis of \cref{RTR-SR1Method}, which can be found in \cite{HuangAbsilGallivan:2014}. For this we need some more assumptions, which are among others also specific for the Riemannian setup. \\
For this we need some more assumptions, which are among others also specific for the Riemannian setup. \\
We assume throughout that $f \in C^2$ and we denote by $\Omega$ the sublevel set of $x_0$, i.e.
\begin{equation*}
    \Omega = \{ x \in \mathcal{M} \colon \ f(x) \leq f(x_0) \}.
\end{equation*}
Furthermore, we assume for the retraction $\retractionSymbol$ that there exists $\mu > 0$ and $\delta_{\mu} > 0$ such that
\begin{equation}\label{RetractionAssumption}
    \lVert \xi_x \rVert_x \geq \mu \ \operatorname{dist}(x, \retract{x}(\xi_x)) \text{ for all } x \in \Omega, \text{ for all } \xi_x \in \tangent{x}, \ \lVert \xi_x \rVert_x \leq \delta_{\mu}.
\end{equation}
Such a condition is instrumental in the global convergence analysis of Riemannian trust-region schemes \cite[p.~7]{HuangAbsilGallivan:2014}. \\
As in the Euclidean case we require that the trust-region subproblem \cref{Riemanniantrsubproblem} is solved accurately enough, which means that there exist positive constants $\sigma_1$ and $\sigma_2$, sucht that 
\begin{equation}\label{RiemannianAccuracy1}
    m_k(0_{x_k}) - m_k(s_k) \geq \sigma_1 \ \lVert \operatorname{grad} f(x_k) \rVert_{x_k} \ \min \{ \Delta_k, \sigma_2 \ \frac{\lVert \operatorname{grad} f(x_k) \rVert_{x_k}}{\lVert \mathcal{H}_k \rVert} \}
\end{equation} 
holds, and that exists a constant $\theta > 0$, such that 
\begin{equation}\label{RiemannianAccuracy2}
    \mathcal{H}_k [s_k] = - \operatorname{grad} f(x_k) + \delta_k \text{ with } \delta_k \in \tangent{x_k}, \ \lVert \delta_k \rVert_{x_k} \leq \lVert \operatorname{grad} f(x_k) \rVert^{1 + \theta}_{x_k}, \text{ whenever } \lVert s_k \rVert_{x_k} < 0.8 \ \Delta_k,
\end{equation}
holds. These conditions are generalizations of \cref{accuracy1} and \cref{accuracy2}. The condition \cref{RiemannianAccuracy2} remains weaker than condition \cref{accuracy2}. The purpose of introducing $\delta_k$ in \cref{RiemannianAccuracy2} is to encompass stopping criteria such as \cite[(7.10)]{AbsilMahonySepulchre:2008} that do not require the computation of an exact solution of the trust-region subproblem \cref{Riemanniantrsubproblem}. We point out in particular that \cref{RiemannianAccuracy1} and \cref{RiemannianAccuracy2} hold if the approximate solution of \cref{Riemanniantrsubproblem} is obtained from \cite[Algorithm~11]{AbsilMahonySepulchre:2008}, the truncated conjugate-gradient method generalized for the Riemannian setup \cite[p.~7]{HuangAbsilGallivan:2014}. \\
The next assumption corresponds to the second in \cref{AssumptionsGlobalConvergence}
\begin{assumption}[{\cite[Assumption~3.1]{HuangAbsilGallivan:2014}}]\label{RiemannianAssumptionsGlobalConvergence} 
    The sequence of linear operators $\{ \mathcal{H}^{\mathrm{SR1}}_k \}_k$ is bounded by a constant $M$ such that $\lVert \mathcal{H}^{\mathrm{SR1}}_k \rVert \leq M$ for all $k$.
\end{assumption}
With all these assumptions, the global convergence of \cref{RTR-SR1Method} can be shown. The first two statements are based on the results in \cite[7.4.1~Global~convergence]{AbsilMahonySepulchre:2008}, which deal with the global convergence analysis of general trust-region methods on Riemannian manifolds (i.e. $\mathcal{H}_k$ in \cref{RiemannianQuadraticModel} is just a self-adjoint operator, which approximates the $\operatorname{Hess} f(x_k)$ sufficiently good), and the third statement generalizes \cref{GlobalConvergence}:
\begin{theorem}[{\cite[Theorem~3.1]{HuangAbsilGallivan:2014}}] \label{RiemannianGlobalConvergence}
    \begin{enumerate}
        \item If $f$ is bounded below on the sublevel set $\Omega$, \cref{RiemannianAssumptionsGlobalConvergence} holds, condition \cref{RiemannianAccuracy1} holds, and \cref{RetractionAssumption} is satisfied then $\lim_{k \rightarrow \infty} \operatorname{grad} f(x_k) = 0$.
        \item If $\mathcal{M}$ is compact, Assumption 3.1 holds, and \cref{RiemannianAccuracy1} holds then $\lim_{k \rightarrow \infty} \operatorname{grad} f(x_k) = 0$, $\{ x_k \}_k$ has at least one limit point, and every limit point of $\{ x_k \}_k$ is a stationary point of $f$.
        \item If the sublevel set $\Omega$ is compact, $f$ has a unique stationary point $x^*$ in $\Omega$, Assumption 3.1 holds, condition \cref{RiemannianAccuracy1} holds, and \cref{RetractionAssumption} is satisfied then $\{ x_k \}_k$ converges to $x^*$.
    \end{enumerate}
\end{theorem}
The local convergence analysis in \cite{HuangAbsilGallivan:2014} can be viewed as a Riemannian generalization of the local convergence analysis in \cite{ByrdKhalfanSchnabel:1996}. The derivation of the results was subject to some hurdles that had to be overcome, which is why several preparation lemmata were used, that are of potential interest in the broader context of Riemannian optimization (). We summarize the required assumptions:
\begin{assumption}[{\cite[Assumptions~3.2+3.3+3.4+3.5+3.6]{HuangAbsilGallivan:2014}}]\label{RiemannianAssumptionsLocalConvergence} \ \\[-1.5\baselineskip]
    \begin{enumerate}
        \item We assume that $\{ x_k \}_k$ converges to a point $x^*$.
        \item We let $\mathcal{U}_{trn}$ be a totally retractive neighborhood of $x^*$. This means that there is $\delta_{trn} > 0$ such that, for each $y \in \mathcal{U}_{trn}$, we have that $\retract{y}(\mathcal{B}(0_y, \delta_{trn})) \supseteq \mathcal{U}_{trn}$ and $\retract{y}(\cdot)$ is a diffeomorphism on $\mathcal{B}(0_y, \delta_{trn})$, where $\mathcal{B}(0_y, \delta_{trn})$ denotes the ball of radius $\delta_{trn}$ in $\tangent{y}$ centered at the origin $0_y$. We assume without loss of generality that $\{ x_k \}_k \subset \mathcal{U}_{trn}$.
        \item The point $x^*$ is a nondegenerate local minimizer of $f$. In other words, $\operatorname{grad} f(x^*) = 0$ and $\operatorname{Hess} f(x^*)$ is positive definite.
        \item There exists a constant $c$ such that for all $x, y \in \mathcal{U}_{trn}$, \begin{equation*} \lVert \operatorname{Hess} f(y) - \mathrm{T}^{S}_{x, \eta_x} \circ \operatorname{Hess} f(x) \circ {\mathrm{T}^{S}_{x, \eta_x}}^{-1} \rVert \leq c \ \operatorname{dist}(x,y), \end{equation*} where $\eta_x = {\retractionSymbol_{x}}^{-1}(y)$.
        \item There exists a constant $c_0$ such that for all $x, y \in \mathcal{U}_{trn}$, all $\xi_x \in \tangent{x}$ with $\retract{x}(\xi_x) \in \mathcal{U}_{trn}$, and all $\xi_y \in \tangent{y}$ with $\retract{y}(\xi_y) \in \mathcal{U}_{trn}$, it holds that \begin{equation*} \lVert \operatorname{Hess} \hat{f}_y(\xi_y) - \mathrm{T}^{S}_{x, \eta_x} \circ \operatorname{Hess} \hat{f}_x(\xi_x) \circ {\mathrm{T}^{S}_{x, \eta_x}}^{-1} \rVert \leq c_0 \ (\lVert \xi_x \rVert_x + \lVert \xi_y \rVert_y + \lVert \eta_x \rVert_x), \end{equation*} where $\eta_x = {\retractionSymbol_{x}}^{-1}(y)$, $\hat{f}_x(\cdot) = f \circ \retract{x}(\cdot)$ and $\hat{f}_y(\cdot) = f \circ \retract{y}(\cdot)$.
        \item For each iteration \cref{RiemannianSafeguard} holds.
        \item There exists $N$ such that, for all $k \geq N$ and all $t \in [0, 1]$, it holds that $\retract{x_k}(t s_k) \in \mathcal{U}_{trn}$.
    \end{enumerate}
\end{assumption}
With all these assumptions, the $n + 1$-step q-superlinear convergence of \cref{RTR-SR1Method} can be shown. The following theorem is a generalization of \cref{LocalConvergence}:
\begin{theorem}[{\cite[Theorem~2.7.]{HuangAbsilGallivan:2014}}] \label{RiemannianLocalConvergence}
    If \cref{RiemannianAssumptionsGlobalConvergence} and \cref{RiemannianAssumptionsLocalConvergence} hold and the subproblem is solved accurately enough for \cref{RiemannianAccuracy1} and \cref{RiemannianAccuracy2} to hold then, the sequence $\{ x_k \}_k$ generated by \cref{RTR-SR1Method} is $n + 1$-step q-superlinear (where $n$ denotes the dimension of the manifold $\mathcal{M}$); i.e.,
    \begin{equation}
        \lim_{k \rightarrow \infty} \frac{\operatorname{dist}(x_{k+n+1}, x^*)}{\operatorname{dist}(x_k, x^*)} = 0.
    \end{equation}
\end{theorem}