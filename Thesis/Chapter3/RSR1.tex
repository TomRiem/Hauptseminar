\section{Riemannian Trust-Region Symmetric Rank-One Method}

%  models, trust-region subproblem, general Trust-Region method, Vector transport, flat, quasi-Newton operator, SR1 Update (unique), safeguard, algorithm, convergence

We will first look at Trust-Region methods on Riemannian manifolds in general. To do this, we must construct a quadratic model for our objective function, which is defined on a Riemannian manifold $\mathcal{M}$. But this is not, in general, a Euclidean space, which makes the situation quite difficult. To avoid this problem, we choose a retraction $\retractionSymbol$, which provides a way to pull back the objective function on the manifold to a objective function on the tangent space. \\
Given a objective function $f \colon \mathcal{M} \to \mathbb{R}$ and a current iterate $x_k \in \mathcal{M}$, we use $\retract{x_k}$ to locally map the minimization problem for $f$ on $\mathcal{M}$ into a minimization problem for the pullback $\hat{f}_{x_k}$ of $f$ under $\retract{x_k}$ on the tangent space $\tangent{x_k}$,
\begin{align*}
    \hat{f}_{x_k} \colon \tangent{x_k} & \to \mathbb{R} \\
    \xi_{x_k} & \mapsto \hat{f}_{x_k} (\xi_{x_k}) = f(\retract{x_k}(\xi_{x_k})).
\end{align*}
The Riemannian metric turns the tangent space $\tangent{x_k}$ into a Euclidean space endowed with the inner product $g_{x_k}(\cdot, \cdot)$, which enables us to construct a order-2 model of the pullback $\hat{f}_{x_k}$:
\begin{align*}
    \hat{m}_k( s ) & = \hat{f}_{x_k}(0_{x_k}) + \mathrm{D} \, \hat{f}_{x_k}(0_{x_k}) [s] + \frac{1}{2} \mathrm{D} \, \hat{f}_{x_k}(0_{x_k}) [s, s] \\
    & = f(x_k) + g_{x_k}(\operatorname{grad} f(x_k), s ) + \frac{1}{2} g_{x_k}( s, \operatorname{Hess} \hat{f}_{x_k}(0_{x_k}) [s]).
\end{align*}
By \cref{Retraction} we get $\hat{f}_{x_k}(0_{x_k}) = f(\retract{x_k}(0_{x_k})) = f(x_k)$ and since the local rigidity condition \cref{LocalRigidity} holds, we have $\mathrm{D} \, \hat{f}_{x_k}(0_{x_k}) = \mathrm{D} \, f(x_k)$, hence $\operatorname{grad} \hat{f}_{x_k}(0_{x_k}) = \operatorname{grad} f(x_k)$. \\
Finally, we “push forward” the model $\hat{m}_k(s)$ through the retraction $\retract{x_k}$, i.e.
\begin{equation}\label{RiemannianModel}
    m_k(s) = \hat{m}_k(s) \circ \inverseRetract{x_k}
\end{equation}
with
\begin{equation}\label{RiemannianModelTangent}
    \hat{m}_k( s ) = f(x_k) + g_{x_k}(\operatorname{grad} f(x_k), s ) + \frac{1}{2} g_{x_k}( s, \operatorname{Hess} f(x_k) [s]),
\end{equation}
where the quadratic term is given by the Riemannian Hessian of the objective function $f$. \\
In general, the model \cref{RiemannianModel} using \cref{RiemannianModel} is only order 1 because $\operatorname{Hess} \hat{f}_{x_k}(0_{x_k}) \neq \operatorname{Hess} f(x_k)$. Since the Hessian operator of the pullback depends among other things on the chosen retraction, further conditions must be imposed on the retraction so that this equality holds. However, for any retraction, $\operatorname{Hess} \hat{f}_{x^*}(0_{x^*}) \neq \operatorname{Hess} f(x^*)$ holds, when $x^*$ is a stationary point of $f$, i.e. $\operatorname{grad} f(x^*) = 0_{x^*}$ \cite[p.~138-139]{AbsilMahonySepulchre:2008}. \\
However, this is of no importance for now, since we only consider quadratic models of the pullback $\hat{f}_{x_k}$ in general, which means 
\begin{equation}\label{RiemannianQuadraticModel}
    \hat{m}_k( s ) = f(x_k) + g_{x_k}(\operatorname{grad} f(x_k), s ) + \frac{1}{2} g_{x_k}( s, \mathcal{H}_k [s]),
\end{equation}
where $\mathcal{H}_k$ is some linear self-adjoint operator on $\tangent{x_k}$. This model \cref{RiemannianQuadraticModel} is easier to handle than the pullback $\hat{f}_{x_k}$ and its purpose is to approximate $\hat{f}_{x_k}$ within a suitable neighbourhood of the zero tangent vector $0_{x_k} \in \tangent{x_k}$ which we refer to as the trust-region. By using the norm induced by the Riemannian metric, we can define the trust-region as the set of all points
\begin{equation}\label{RiemannianTrustRegion}
    \{ s \in \tangent{x_k} \colon \ \lVert s \rVert_{x_k} \leq \Delta_k \}
\end{equation}
where $\Delta_k > 0$ is the trust-region radius. We point out that our trust-region now consists not of all points on the manifold which have the distance less or equal $\Delta_k$ to the current iterate $x_k$, but of all tangent vectors in tangent space at the current iterate $\tangent{x_k}$ whose length is less or equal $\Delta_k$. The chosen retraction $\retractionSymbol$ defines for any iterate $x_k \in \mathcal{M}$, a one-to-one correspondence $\retract{x_k}$ between a neighborhood of $x_k$ in $\mathcal{M}$ and a neighborhood of $0_{x_k}$ in the tangent space $\tangent{x_k}$ (we remember $\retract{x_k}(0_{x_k}) = x_k$) \cite[p.~304]{AbsilBakerGallivan:2007}. But the chosen retraction $\retractionSymbol$ applied to a tangent vector $\xi_{x_k} \in \tangent{x_k}$ generally does not lead to a point $\retract{x_k}(\xi_{x_k})$ whose distance to the starting point $x_k$ is equal to the norm of the tangent vector $\xi_{x_k}$, i.e. $\operatorname{dist}(x_k, \retract{x_k}(\xi_{x_k})) \neq \lVert \xi_{x_k} \rVert_{x_k}$. \\
Next, we compute the step $s_k \in \tangent{x_k}$ as an (approximate) solution of the trust-region subproblem given by the model \cref{RiemannianQuadraticModel} and the trust-region \cref{RiemannianTrustRegion}, i.e.
\begin{equation}\label{Riemanniantrsubproblem}
    s_k = \arg \min_{\lVert s \rVert_{x_k} \leq \Delta_k} \hat{m}_k( s ) \in \tangent{x_k}.
\end{equation}
Since $\tangent{x_k}$ is a Euclidean space, it is possible to adapt classical methods in $R^n$ to compute a approximate minimizer of the trust-region subproblem \cref{Riemanniantrsubproblem} \cite[p.~304]{AbsilBakerGallivan:2007}. A possible method for this is given by \cite[Algorithm~11]{AbsilMahonySepulchre:2008} which is based on a generalization of the truncated conjugate-gradient method into the Riemannian setup. \\
This minimizer $s_k$ is then retracted back from $\tangent{x_k}$ to $\mathcal{M}$, i.e.
\begin{equation*}
    \widetilde{x}_{k+1} = \retract{x_k}(s_k).
\end{equation*}
This point is a candidate for the new iterate $x_{k+1}$. The decisions on accepting or rejecting the candidate $\widetilde{x}_{k+1}$ and on selecting the new trust-region radius $\Delta_k$ are based on the quotient
\begin{equation*}
    \rho_k = \frac{\hat{f}_{x_k}(0_{x_k}) - \hat{f}_{x_k}(s_k)}{\hat{m}_k(0_{x_k}) - \hat{m}_k(s_k)} = \frac{f(x_k) - f(\retract{x_k}(s_k))}{\hat{m}_k(0_{x_k}) - \hat{m}_k(s_k)} = \frac{f(x_k) - f(\widetilde{x}_{k+1})}{\hat{m}_k(0_{x_k}) - \hat{m}_k(s_k)}.
\end{equation*}
The decisions as well as the resulting updates concerning the candidate and the trust-region radius follow the same heuristics as in Euclidean. \\ % Schönerer satz bitte
We point out that this “pullback-solve-retract” distinguishes the Riemannian Trust-Region approach from the Euclidean Trust-Region methods, which only require the “solve” part since they live in $\mathbb{R}^n$. On a manifold, using the pullback $\hat{f}_{x_k}$ of the objective function $f$ makes it possible to locally fall back to a friendly Euclidean world (the tangent space $\tangent{x_k}$) where classical techniques can be applied, and the retraction $\retract{x_k}$ brings the result back to the manifold $\mathcal{M}$. A difficulty, from an analysis perspective, is that the Riemannian Trust-Region approach does not deal with a unique objective function $f$,
but rather with a succession of different pullbacks $\hat{f}_{x_k}$ of the objetcive function \cite[p.~305]{AbsilBakerGallivan:2007}. \\

Let us now turn to the quadratic term $\mathcal{H}_k \colon \tangent{x_k} \to \tangent{x_k}$ in \cref{RiemannianQuadraticModel}. It can be shown, if $\mathcal{H}_k$ is a sufficiently good approximation of the Hessian operator $\operatorname{Hess} f(x_k)$ and under further (strong) assumptions, among others on the retraction $\retractionSymbol$, that the the sequence $\{ x_k \}_k$ from the resulting method converges q-superlinearly (see \cite[Theorem~7.4.11]{AbsilMahonySepulchre:2008}). A possible choice would be the Riemannian Hessian $\mathcal{H}_k = \operatorname{Hess} f(x_k)$ and for $\retractionSymbol$ the exponential map $\expOp$ (see \cite[p.~102]{AbsilMahonySepulchre:2008}). But as in the Euclidean case, the application of $\operatorname{Hess} f(x_k)$ can be very costly or $\operatorname{Hess} f(x_k)$ doesn't even exist and for $\expOp$ these can also be reasons why one does not use this method. Therefore, we are looking for an approximation of $\operatorname{Hess} f(x_k)$ that is easy to compute but still produces a fast rate of convergence. This leads us to genralizing the SR1 update \cref{directSR1formula} for the Riemannian setup. \\
This means we want to approximate the action of the Riemannian Hessian $\operatorname{Hess} f(x_k)$ with a linear self-adjoint operator $\mathcal{H}^\mathrm{SR1}_k \colon \tangent{x_k} \to \tangent{x_k}$, which is updated (in each iteration) with information about the curvature obtained by $s_k, \operatorname{grad} f(x_k) \in \tangent{x_k}$ and $\operatorname{grad} f(\widetilde{x}_{k+1}) \in \tangent{\widetilde{x}_{k+1}}$ to a new operator $\mathcal{H}^\mathrm{SR1}_{k+1} \colon \tangent{x_{k+1}} \to \tangent{x_{k+1}}$, which acts on the tangent space at the upcomming iterate $x_{k+1}$. To be able to use the information from different tangent spaces, we need a tool that transports tangent vectors and preferably operators between them:
\begin{definition}[{\cite[Definition~8.1.1]{AbsilMahonySepulchre:2008}}]\label{VectorTransport}
    A vector transport on a manifold $\mathcal{M}$ is a smooth map 
    \begin{align*}
        \vectorTransportSymbol \colon \; \tangent{} \oplus \tangent{} & \to \tangent{} \\
        (\eta_x, \xi_x) & \mapsto \vectorTransportDir{x}{\eta_x}(\xi_x)
    \end{align*}    
    satisfying the following properties for all $x \in \mathcal{M}$:
    \begin{enumerate}
        \item (Associated retraction) There exists a retraction $\retractionSymbol$, called the retraction associated with $\vectorTransportSymbol$, such that the following diagram commutes \begin{equation*}
        \begin{xy} \xymatrix{(\eta_x, \xi_x) \ar[r]^{\vectorTransportSymbol} \ar[d] & \vectorTransportDir{x}{\eta_x}(\xi_x) \ar[d]^\pi \\ \eta_x \ar[r]_{\retractionSymbol} & \pi (\vectorTransportDir{x}{\eta_x}(\xi_x)) } \end{xy} \end{equation*} where $\pi (\vectorTransportDir{x}{\eta_x}(\xi_x))$ denotes the foot of the tangent vector $\vectorTransportDir{x}{\eta_x}(\xi_x)$. \label{VectorTransport1}
        \item (Consistency) $\vectorTransportDir{x}{0_x}(\xi_x) = \xi_x$ for all $\xi_x \in \tangent{x}$; 
        \item (Linearity) $\vectorTransportDir{x}{\eta_x}(a \xi_x + b \zeta_x) = a \vectorTransportDir{x}{\eta_x}(\xi_x) + b \vectorTransportDir{x}{\eta_x}(\zeta_x)$.
    \end{enumerate}
\end{definition}
A vector transport $\vectorTransportSymbol^S \colon \; \tangent{} \oplus \tangent{} \to \tangent{}$ with associated retraction $\retractionSymbol$ is called isometric if it satisfies for all $x \in \mathcal{M}$
\begin{equation}\label{IsometricVectorTransport}
    g_{\retract{x}(\eta_x)}(\vectorTransportDir{x}{\eta_x}(\xi_x)[S], \vectorTransportDir{x}{\eta_x}(\zeta_x)[S]) = g_x (\xi_x, \zeta_x)
\end{equation}
for all $\eta_x, \xi_x, \zeta_x \in \tangent{x}$ \cite[p.~10]{Huang:2013}. We use $\vectorTransportSymbol^S$ to denote an isometric vector transport. \\
Thus, we are now able to summarize the information for the update in one tangent space. Since, as in the Euclidean case, we want to update $\mathcal{H}^\mathrm{SR1}_k$ before deciding whether or not to accept the candidate $\widetilde{x}_{k+1}$, we use the tangent space at the current iterate $\tangent{x_k}$. For this we define $y_k = {\mathrm{T}^{S}_{x_k, s_k}}^{-1} ( \operatorname{grad}f(\widetilde{x}_{k+1}) ) - \operatorname{grad}f(x_k) \in \tangent{x_k}$, where the associated retraction of $\vectorTransportSymbol^S$ is our chosen retraction $\retractionSymbol$. \\
To be able to create rank-one operators, we introduce the musical isomorphism $\flat \colon \; \tangent{x_k} \ni \eta_{x_k} \mapsto \eta^{\flat}_{x_k} \in \cotangent{x_k}$ (see \cite[p.~6]{BergmannHerzogLouzeiroSilvaTenbrinckVidalNunez:2020:1}). Put simply, it means: $\eta^{\flat}_{x_k} \in \cotangent{x_k}$ represents the flat of $\eta_{x_k} \in \tangent{x_k}$, i.e., $\eta^{\flat}_{x_k} \colon \; \tangent{x_k} \to \mathbb{R}, \; \xi_{x_k} \mapsto \eta^{\flat}_{x_k}[\xi_{x_k}] = g_{x_k} (\eta_{x_k}, \xi_{x_k})$. This generalizes the notion of the transpose from the Euclidean case. It can be shown that $\eta_{x_k} \eta^{\flat}_{x_k} \tangent{x_k} \to \tangent{x_k}$ is a linear self-adjoint positive definite rank-one operator. \\
With $s_k \in \tangent{x_k}$, which generalizes the connection between the current iterate $x_k$ and the candidate $\widetilde{x}_{k+1}$, $y_k \in \tangent{x_k}$, which generalizes the difference of the gradients at $x_k$ and $\widetilde{x}_{k+1}$, and by introducing the notion of the flat $\flat$, we can now define a self-adjoint rank-one, also short SR1, update for operators on the tangent space at the current iterate $\tangent{x_k}$:
\begin{equation}\label{RiemannianDirectSR1formula}
    \widetilde{\mathcal{H}}^{SR1}_{k+1} [\cdot] = \mathcal{H}^\mathrm{SR1}_k [\cdot] + \frac{(y_k - \mathcal{H}^\mathrm{SR1}_k [s_k]) (y_k - \mathcal{H}^\mathrm{SR1}_k [s_k])^{\flat} [\cdot] }{(y_k - \mathcal{H}^\mathrm{SR1}_k [s_k])^{\flat} [s_k]}.
\end{equation}
We see immediately that \cref{RiemannianDirectSR1formula} creates a self-adjoint operator if $\mathcal{H}^\mathrm{SR1}_k$ is self-adjoint. But even if $\mathcal{H}^\mathrm{SR1}_k$ is positive definite, $\widetilde{\mathcal{H}}^{SR1}_{k+1}$ may not have the same property. If and only if $(y_k - \mathcal{H}^\mathrm{SR1}_k [s_k])^{\flat} [s_k] > 0$, the SR1 update retains positive definiteness. \\
As in the Euclidean case, the update \cref{RiemannianDirectSR1formula} has a close connection to Riemannian quasi-Newton methods, where it is required that the approximation of the Hessian operator $\operatorname{Hess} f(x_{k+1})$ generated by the corresponding update satisfies some kind of Riemann quasi-Newton equation (for more details see \cite[Chapter~2]{Huang:2013}). For \cref{RiemannianDirectSR1formula}, it can be shown that 
\begin{equation*}
    \widetilde{\mathcal{H}}^{SR1}_{k+1} [s_k] = y_k 
\end{equation*}
holds, which generalizes \cref{quasi-NewtonEquation} in this context. \\


There is also the possibility that this method can break down if the numerator in \cref{RiemannianDirectSR1formula} is equal to zero. In order to overcome this, can be transferred to the Riemannian setup. This means that \cref{RiemannianDirectSR1formula} is only executed if
\begin{equation}\label{RiemannianSafeguard}
    \lvert g_{x_{k+1}}(y_k - \widetilde{\mathcal{H}}^\mathrm{SR1}_k [s_k], s_k) \lvert \; \geq \; r \; \lVert s_k \rVert_{x_{k+1}} \lVert y_k - \widetilde{\mathcal{H}}^\mathrm{SR1}_k [s_k] \rVert_{x_{k+1}} 
\end{equation}
holds, where $g_{x_{k+1}}(\cdot, \cdot) \colon \; \tangent{x_{k+1}} \times \tangent{x_{k+1}} \to \mathbb{R}$ is the inner product on $\tangent{x_{k+1}}$ (see \cite[p.~6]{BergmannHerzogLouzeiroSilvaTenbrinckVidalNunez:2020:1}) and $ \lVert \cdot \rVert_{x_{k+1}} = \sqrt{g_{x_{k+1}}(\cdot, \cdot)}$ denotes the resulting norm on $\tangent{x_{k+1}}$. The factor $r$ is again in the interval $(0,1)$. \\















The main drawback of the SR1 update formula is that the denominator in \cref{directSR1formula} or   can vanish. In fact, even when the objective function $f$ is convex and quadratic, there may be steps on which there is no symmetric rank-one update that satisfies the quasi-Newton equation \cref{quasi-NewtonEquation}. This disadvantage results in serious numerical difficulties, which restrict the applications of it. Nevertheless, the SR1 update formula has the following advantages:
For the vanishing denominator in \cref{directSR1formula} we introduce a strategy to prevent a method using the SR1 update from breaking down. It has been observed in practice that it performs well simply by skipping the update if the denominator is small. More specifically, the update \cref{directSR1formula} is applied only if 
\begin{equation}
    \lvert (y_k - H^\mathrm{SR1}_k s_k)^{\mathrm{T}} s_k \lvert \; \geq \; r \; \lVert s_k \rVert \lVert y_k - H^\mathrm{SR1}_k s_k \rVert 
\end{equation}
holds, where $r \in (0, 1)$ is a small number, e.g. $r = 10^{−8}$. Most implementations of the SR1 update use a skipping rule of this kind. The condition $(y_k - H^\mathrm{SR1}_k s_k)^{\mathrm{T}} s_k \approx 0$ occurs infrequently, since it requires certain vectors to be aligned in a specific way. When it occurs, skipping the update appears to have no negative effects on the iteration, since the skipping condition implies that $s^{\mathrm{T}}_k \tilde{G} s_k \approx s^{\mathrm{T}}_k H^\mathrm{SR1}_k s_k$, where $\tilde{G}$ is the average Hessian over the last step, meaning that the curvature of $H^\mathrm{SR1}_k$ along $s_k$ is already correct. \\






\begin{algorithm}[H]
    \caption{Riemannian Trust-Region Symmetric Rank-One Method}\label{RTR-SR1Method}
    \begin{algorithmic}[1]
        \State Riemannian manifold $(\mathcal{M}, g)$; isometric vector transport $\vectorTransportSymbol^S$ with $\retractionSymbol$ as associated retraction; continuously differentiable real-valued function $f$ on $\mathcal{M}$, bounded below; initial iterate $x_0 \in \mathcal{M}$; initial self-adjoint operator $\mathcal{H}^\mathrm{SR1}_0 \colon \; \tangent{x_0} \to \tangent{x_0}$; initial trust-region radius $\Delta_0 > 0$; safeguard constant $\nu \in (0,1)$; rejection boundary $\rho^{\prime} \in (0, 0.1)$; trust-region reduction factor $\tau_1 \in (0,1)$; trust-region magnification factor $\tau_2 > 1$; convergence tolerance $\varepsilon > 0$. Set $k = 0$.
        \While{$\lVert \nabla f(x_k) \rVert > \varepsilon$}
            \State Obtain $s_k$ (approximately) solving \cref{Riemanniantrsubproblem} using $\mathcal{H}^\mathrm{SR1}_k$.
            \State Set $\widetilde{x}_{k+1} = \retract{x_k}(s_k)$ and ${T^{S}_{x_k, s_k}}^{-1} ( \operatorname{grad}f(\widetilde{x}_{k+1}) ) - \operatorname{grad}f(x_k) \in \tangent{x_k}$.
            \If{\cref{RiemannianSafeguard} holds}
                \State Compute $\widetilde{\mathcal{H}}^\mathrm{SR1}_{k+1} \colon \tangent{x_{k}} \to \tangent{x_{k}}$ by means of \cref{RiemannianDirectSR1formula}.
			\Else 
				\State Set $\widetilde{\mathcal{H}}^\mathrm{SR1}_{k+1} = \mathcal{H}^\mathrm{SR1}_k$.
            \EndIf 
            \State Compute $\rho_k = \frac{f(x_k) - f(\widetilde{x}_{k+1})}{m_k(0) - m_k(s_k)}$.
            \If{$\rho_k > \rho^{\prime}$}
                \State Set $x_{k+1} = \widetilde{x}_{k+1}$ and $\mathcal{H}^\mathrm{SR1}_{k+1} = T^{S}_{x_k, s_k} \circ \widetilde{\mathcal{H}}^\mathrm{SR1}_{k+1} \circ  {T^{S}_{x_k, s_k}}^{-1} \colon \tangent{x_{k+1}} \to \tangent{x_{k+1}}$.
			\Else 
				\State Set $x_{k+1} = x_k$ and $\mathcal{H}^\mathrm{SR1}_{k+1} = \widetilde{\mathcal{H}}^\mathrm{SR1}_{k+1} \colon \tangent{x_{k}} \to \tangent{x_{k}}$.
            \EndIf 
            \If{$\rho_k > 0.75$}
                \If{$\lVert s_k \rVert \geq 0.8 \ \Delta_k$}
                    \State Set $\Delta_{k+1} = \tau_2 \ \Delta_k$.
                \Else 
                    \State Set $\Delta_{k+1} = \Delta_k$.
                \EndIf 
			\Else 
                \If{$\rho_k < 0.1$}
                    \State Set $\Delta_{k+1} = \tau_1 \ \Delta_k$.
                \Else 
                    \State Set $\Delta_{k+1} = \Delta_k$.
                \EndIf 
            \EndIf 
            \State Set $k = k+1$.
        \EndWhile
        \State \textbf{Return} $x_k$.
    \end{algorithmic}
\end{algorithm}

Moreover, we assume throughout that $f \in C^2$.

We let $\Omega$ denote the sublevel set of $x_0$, i.e.,
\begin{equation*}
    \Omega = \{ x \in \mathcal{M} \colon \ f(x) \leq f(x_0) \}.
\end{equation*}

In some results, we will assume for the retraction $\retractionSymbol$ that there exists $\mu > 0$ and $\delta_{\mu} > 0$ such that
\begin{equation}\label{RetractionAssumption}
    \lVert \xi_x \rVert_x \geq \mu \ \operatorname{dist}(x, \retract{x}(\xi_x)) \text{ for all } x \in \Omega, \text{ for all } \xi_x \in \tangent{x}, \ \lVert \xi_x \rVert_x \leq \delta_{\mu}.
\end{equation}
Such a condition is instrumental in the global convergence analysis of Riemannian trust-region schemes.

We will often require that the trust-region subproblem is solved accurately enough that, for positive constants $\sigma_1$ and $\sigma_2$,
\begin{equation}\label{RiemannianAccuracy1}
    m_k(0_{x_k}) - m_k(s_k) \geq \sigma_1 \ \lVert \operatorname{grad} f(x_k) \rVert_{x_k} \ \min \{ \Delta_k, \sigma_2 \ \frac{\lVert \operatorname{grad} f(x_k) \rVert_{x_k}}{\lVert \mathcal{H}_k \rVert} \}
\end{equation} 
and that
\begin{equation}\label{RiemannianAccuracy2}
    \mathcal{H}_k [s_k] = - \operatorname{grad} f(x_k) + \delta_k \text{ with } \delta_k \in \tangent{x_k}, \ \lVert \delta_k \rVert_{x_k} \leq \lVert \operatorname{grad} f(x_k) \rVert^{1 + \theta}_{x_k}, \text{ whenever } \lVert s_k \rVert_{x_k} < 0.8 \ \Delta_k,
\end{equation}
where $\theta > 0$ is a constant. These conditions are generalizations of \cref{accuracy1} and \cref{accuracy2}. The condition \cref{RiemannianAccuracy2} remains weaker than condition \cref{accuracy2}. The purpose of introducing $\delta_k$ in \cref{RiemannianAccuracy2} is to encompass stopping criteria such as \cite[(7.10)]{AbsilMahonySepulchre:2008} that do not require the computation of an exact solution of the trust-region subproblem. We point out in particular that \cref{RiemannianAccuracy1} and \cref{RiemannianAccuracy2} hold if the approximate solution of the trust-region subproblem is obtained from the truncated CG method, described in \cite[§~7.3.2]{AbsilMahonySepulchre:2008} in the Riemannian context.


\begin{assumption}[{\cite[Assumption~3.1]{HuangAbsilGallivan:2014}}]\label{RiemannianAssumptionsGlobalConvergence} 
    The sequence of linear operators $\{ \mathcal{H}^{\mathrm{SR1}}_k \}_k$ is bounded by a constant $M$ such that $\lVert \mathcal{H}^{\mathrm{SR1}}_k \rVert \leq M$ for all $k$.
\end{assumption}

\begin{theorem}[{\cite[Theorem~3.1]{HuangAbsilGallivan:2014}}] \label{RiemannianGlobalConvergence}
    \begin{enumerate}
        \item If $f$ is bounded below on the sublevel set $\Omega$, \cref{RiemannianAssumptionsGlobalConvergence} holds, condition \cref{RiemannianAccuracy1} holds, and \cref{RetractionAssumption} is satisfied then $\lim_{k \rightarrow \infty} \operatorname{grad} f(x_k) = 0$.
        \item If $\mathcal{M}$ is compact, Assumption 3.1 holds, and \cref{RiemannianAccuracy1} holds then $\lim_{k \rightarrow \infty} \operatorname{grad} f(x_k) = 0$, $\{ x_k \}_k$ has at least one limit point, and every limit point of $\{ x_k \}_k$ is a stationary point of $f$.
        \item If the sublevel set $\Omega$ is compact, $f$ has a unique stationary point $x^*$ in $\Omega$, Assumption 3.1 holds, condition \cref{RiemannianAccuracy1} holds, and \cref{RetractionAssumption} is satisfied then $\{ x_k \}_k$ converges to $x^*$.
    \end{enumerate}
\end{theorem}




\begin{assumption}[{\cite[Assumptions~3.2+3.3+3.4+3.5+3.6]{HuangAbsilGallivan:2014}}]\label{RiemannianAssumptionsLocalConvergence} \ \\[-1.5\baselineskip]
    \begin{enumerate}
        \item We assume that $\{ x_k \}_k$ converges to a point $x^*$.
        \item We let $\mathcal{U}_{trn}$ be a totally retractive neighborhood of $x^*$. This means that there is $\delta_{trn} > 0$ such that, for each $y \in \mathcal{U}_{trn}$, we have that $\retract{y}(\mathcal{B}(0_y, \delta_{trn})) \supseteq \mathcal{U}_{trn}$ and $\retract{y}(\cdot)$ is a diffeomorphism on $\mathcal{B}(0_y, \delta_{trn})$, where $\mathcal{B}(0_y, \delta_{trn})$ denotes the ball of radius $\delta_{trn}$ in $\tangent{y}$ centered at the origin $0_y$. We assume without loss of generality that $\{ x_k \}_k \subset \mathcal{U}_{trn}$.
        \item The point $x^*$ is a nondegenerate local minimizer of $f$. In other words, $\operatorname{grad} f(x^*) = 0$ and $\operatorname{Hess} f(x^*)$ is positive definite.
        \item There exists a constant $c$ such that for all $x, y \in \mathcal{U}_{trn}$, \begin{equation*} \lVert \operatorname{Hess} f(y) - \mathrm{T}^{S}_{x, \eta_x} \circ \operatorname{Hess} f(x) \circ {\mathrm{T}^{S}_{x, \eta_x}}^{-1} \rVert \leq c \ \operatorname{dist}(x,y), \end{equation*} where $\eta_x = {\retractionSymbol_{x}}^{-1}(y)$.
        \item There exists a constant $c_0$ such that for all $x, y \in \mathcal{U}_{trn}$, all $\xi_x \in \tangent{x}$ with $\retract{x}(\xi_x) \in \mathcal{U}_{trn}$, and all $\xi_y \in \tangent{y}$ with $\retract{y}(\xi_y) \in \mathcal{U}_{trn}$, it holds that \begin{equation*} \lVert \operatorname{Hess} \hat{f}_y(\xi_y) - \mathrm{T}^{S}_{x, \eta_x} \circ \operatorname{Hess} \hat{f}_x(\xi_x) \circ {\mathrm{T}^{S}_{x, \eta_x}}^{-1} \rVert \leq c_0 \ (\lVert \xi_x \rVert_x + \lVert \xi_y \rVert_y + \lVert \eta_x \rVert_x), \end{equation*} where $\eta_x = {\retractionSymbol_{x}}^{-1}(y)$, $\hat{f}_x(\cdot) = f \circ \retract{x}(\cdot)$ and $\hat{f}_y(\cdot) = f \circ \retract{y}(\cdot)$.
        \item For each iteration \cref{RiemannianSafeguard} holds.
        \item There exists $N$ such that, for all $k \geq N$ and all $t \in [0, 1]$, it holds that $\retract{x_k}(t s_k) \in \mathcal{U}_{trn}$.
    \end{enumerate}
\end{assumption}

\begin{theorem}[{\cite[Theorem~2.7.]{HuangAbsilGallivan:2014}}] \label{RiemannianLocalConvergence}
    If \cref{RiemannianAssumptionsGlobalConvergence} and \cref{RiemannianAssumptionsLocalConvergence} hold and the subproblem is solved accurately enough for \cref{RiemannianAccuracy1} and \cref{RiemannianAccuracy2} to hold then, the sequence $\{ x_k \}_k$ generated by \cref{RTR-SR1Method} is $n + 1$-step q-superlinear (where $n$ denotes the dimension of the manifold $\mathcal{M}$); i.e.,
    \begin{equation}
        \lim_{k \rightarrow \infty} \frac{\operatorname{dist}(x_{k+n+1}, x^*)}{\operatorname{dist}(x_k, x^*)} = 0.
    \end{equation}
\end{theorem}