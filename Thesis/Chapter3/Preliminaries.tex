\section{Preliminaries}

Before we enter the theory about the Trust-Region Symmetric Rank-One method on Riemannian manifolds, we introduce certain basic principles and aspects in this section, which will be needed later. \\

From now on we consider Riemannian optimization problems, which consider finding an optimum of a real-valued function $f$ defined on a Riemannian manifold $\mathcal{M}$, i.e.
\begin{equation*}
    \min f(x), \quad x \in \mathcal{M}.
\end{equation*}
We assume throughout that $\mathcal{M}$ is a $n$-dimensional geodesically complete Riemannian manifold. We further assume that $\mathcal{M}$ is embedded in a real-valued space (e.g. $\mathcal{M} \subseteq \mathbb{R}^m$) and connected. We assume that $f \colon \ \mathcal{M} \to \mathbb{R}$ is a twice continuously differentiable function, i.e. $f \in C^2(\mathcal{M})$. \\
In order to generalize Trust-Region methods, we have to consider directions on a manifold. For that we introduce tangent vectors. A tangent vector $\xi_x$ at a point $x \in \mathcal{M}$ is a map from $C^{\infty}_x(\mathcal{M})$ (the set of smooth real-valued functions defined on a neighborhood of $x$) to $\mathbb{R}$ such that there exists a curve $\geodesicSymbol$ on $\mathcal{M}$ with $\geodesic<s>(0) = x$, satisfying
\begin{equation*}
    \xi_x (f) = \dot{\geodesicSymbol}(0) \; f = \frac{\mathrm{d}}{\mathrm{d}t} f(\geodesic<s>(t)) \; \big{\vert}_{t=0}
\end{equation*}
for all $f \in C^{\infty}_x(\mathcal{M})$. The set of all tangent vectors at $x \in \mathcal{M}$ forms the tangent space $\tangent{x}$, which is a vector space of dimension $n$. The disjoint union of all tangent spaces, i.e. $\tangent{} = \coprod_{x \in \mathcal{M}} \tangent{x}$, is called the tangent bundle of $\mathcal{M}$. It is a smooth manifold of dimension $2n$ \cite[p.~33-36]{AbsilMahonySepulchre:2008}. \\
On a Riemannian manifold $\mathcal{M}$ we have the so-called Riemannian metric, which is a smoothly varying family of inner products on the tangent spaces. The metric on the tangent space at $x \in \mathcal{M}$ is denoted by $g_x(\cdot, \cdot) \colon \ \tangent{x} \times \tangent{x} \to \mathbb{R}$. The Riemannian metric also induces a norm $\lVert \xi_x \rVert_x = \sqrt{g_x(\xi_x, \xi_x)}$ on each tangent space $\tangent{x}$, $x \in \mathcal{M}$ \cite[p.~45]{AbsilMahonySepulchre:2008}. \\
The length of a curve $\geodesicSymbol \colon \ [a,b] \to \mathcal{M}$ on a Riemannian manifold is defined by
\begin{equation*}
    \mathrm{L}(\geodesicSymbol) = \int^{b}_a \sqrt{ g_{\geodesic<s>(t)} (\dot{\geodesic<s>}(t), \dot{\geodesic<s>}(t)) } \mathrm{d}t.
\end{equation*}
The Riemannian distance of two points $x,y \in \mathcal{M}$ is defined by
\begin{equation*}
    \operatorname{dist}(x,y) = \inf_{\geodesicSymbol \in \Gamma} \mathrm{L}(\geodesicSymbol),
\end{equation*}
where $\Gamma$ is the set of all curves in $\mathcal{M}$ joining the points $x$ and $y$ \cite[p.~46]{AbsilMahonySepulchre:2008}. \\
In the Euclidean case, the gradient provides us with useful information. For a real-valued function $f$, the Riemannian gradient $\operatorname{grad} f(x)$ of $f$ at $x$ is the unique tangent vector such that
\begin{equation*}
    \mathrm{D} \, f(x) [\xi_x] = g_x (\operatorname{grad} f(x), \xi_x), \quad \text{for all } \xi_x \tangent{x},
\end{equation*}
where $\mathrm{D} \, f(x)$ denotes the differential of $f$ at $x$ \cite[p.~46]{AbsilMahonySepulchre:2008}. \\
The Hessian is required in second-order optimization algorithms, such as the Trust-Region Newton method. The Riemannian Hessian represents the covariant derivative of the gradient in the direction of the chosen tangent vector, i.e. the Riemannian Hessian of $f$ at $x$ is a linear operator on $\tangent{x}$ defined by
\begin{equation*}
    \operatorname{Hess} f(x) [\xi_{x}] = \nabla_{\xi_{x}} \operatorname{grad} f(x) \quad \text{for all } \xi_x \in \tangent{x},
\end{equation*}
where $\nabla$ denotes the so-called Levi-Civita connection, which is a unique affine connection on a Riemannian Manifold $\mathcal{M}$ (see \cite[Theorem~5.3.1~(Levi-Civita)]{AbsilMahonySepulchre:2008}). Furthermore, the Riemannian Hessian $\operatorname{Hess} f(x)$ is a self-adjoint operator on each tangent space, i.e. it holds $g_x(\operatorname{Hess} f(x) [\xi_{x}], \eta_x) = g_x(\xi_{x}, \operatorname{Hess} f(x) [\eta_x])$ for all $\xi_x, \eta_x \in \tangent{x}$ and all $x \in \mathcal{M}$ \cite[p.~105]{AbsilMahonySepulchre:2008}. \\
On a manifold, the notion of moving in the direction of a tangent vector, while staying on the manifold, is generalized by the notion of a so-called retraction:
\begin{definition}[{\cite[Definition~4.1.1]{AbsilMahonySepulchre:2008}}]\label{Retraction}
    A retraction on a manifold $\mathcal{M}$ is a smooth map $\retractionSymbol \colon \ \tangent{} \to \mathcal{M}$ with the following properties. Let $\retract{x}(\cdot)$ denote the restriction of $\retractionSymbol$ to $\tangent{x}$: 
    \begin{enumerate}
        \item $\retract{x}(0_x) = x$, where $0_x$ denotes the zero element of $\tangent{x}$. 
        \item With the canonical identification $\tangent{0_x}[\tangent{x}] \simeq \tangent{x}$, $\retract{x}$ satisfies \begin{equation} \mathrm{D} \; \retract{x}(0_x) = \id_{\tangent{x}}, \label{LocalRigidity} \end{equation} where $\id_{\tangent{x}}$ denotes the identity map on $\tangent{x}$.  
    \end{enumerate}
\end{definition}
We assume that the domain of $\retractionSymbol$ is the whole tangent bundle $\tangent{}$. The condition \cref{LocalRigidity} is called local rigidity condition. Equivalently, for every tangent vector $\xi_x \in \tangent{x}$, the curve $\geodesicSymbol \colon \ t \to \retract{x}(t \ \xi_x)$ satisfies $\dot{\geodesicSymbol}(0) = \xi_x$. Moving along this curve $\geodesicSymbol$ is thought of as moving in the direction $\xi_x$ while constrained to the manifold $\mathcal{M}$. The choice of a computationally efficient retraction is an important decision in the design of high-performance numerical algorithms on manifolds \cite[p.~54]{AbsilMahonySepulchre:2008}. \\
Besides turning tangent vectors into points of the manifold, a second important purpose of a retraction is to transform cost functions defined in a neighborhood of $x \in \mathcal{M}$ into cost functions defined on the vector space $\tangent{x}$. Specifically, given a real-valued function $f$ on a manifold $\mathcal{M}$ equipped with a retraction $\retractionSymbol$, we let $\hat{f} = f \circ \retractionSymbol$ denote the pullback of $f$ through $\retractionSymbol$. For $x \in \mathcal{M}$, we let
\begin{equation}\label{pullback}
    \hat{f}_x = f \circ \retract{x} \colon \ \tangent{x} \to \mathbb{R}
\end{equation}
denote the restriction of $\hat{f}$ to $\tangent{x}$, which is now a real-valued function on a vector space. We note that since the local rigidity condition \cref{LocalRigidity} holds, we have $\mathrm{D} \, \hat{f}_x (0_x) = \mathrm{D} \, f(x)$, and since we assume that $\mathcal{M}$ is a Riemannian manifold (and thus $\tangent{x}$ with an inner product), we have 
\begin{equation}\label{PullbackGradient}
    \operatorname{grad} \hat{f}_x (0_x) = \operatorname{grad} f(x)
\end{equation}
\cite[p.~54-56]{AbsilMahonySepulchre:2008}.
