\chapter{The Euclidean Symmetric Rank-One Quasi-Newton Method}

In the Euclidean optimization a key problem is minimizing a real-valued function $f$ over the Euclidean space $\mathbb{R}^n$ ($n \geq 1$), i.e. our focus and efforts are centred on solving 
\begin{equation}\label{OptimizationProblem}
    \min f(x), \quad x \in \mathbb{R}^n
\end{equation}  
where $f \colon \; \mathbb{R}^n \to \mathbb{R}$ is a smooth function. In this chapter we focus on smooth functions, by which we generally mean functions whose second derivatives exist and are continuous or formally $f \in C^2(\mathbb{R}^n)$, unless otherwise stated. \cref{OptimizationProblem} is called a (nonlinear) unconstrained optimization problem. \\
To solve the problem \cref{OptimizationProblem} we want to use quasi-Newton methods, which belong to the class of line search methods. These can be formulated as algorithms where the next iterate is obtained by the iterative update scheme
\begin{equation*}
    x_{k+1} = x_k + \alpha_k d_k.
\end{equation*}
This means these methods start with an initial point $x_0 \in \mathbb{R}^n$ and produce a sequence of iterates $\{x_k\}_k$ that we hope will converge towards a minimum of \cref{OptimizationProblem}. The algorithms follow the strategy of first determining a so-called search direction $d_k \in \mathbb{R}^n$ and then a suitable stepsize $\alpha_k > 0$ is searched for along this search direction $d_k$. \\
In quasi-Newton methods, 
\begin{equation*}
    d_k = -{H_k}^{-1} \nabla f(x_k) = -B_k \nabla f(x_k)
\end{equation*}
is used as search direction, where the matrix $H_k \in \mathbb{R}^{n \times n}$ approximates the action of the objective's Hessian $\nabla^{2} f(\cdot)$ at the current iterate $x_k$ in the direction of $s_k$ and $B_k = {H_k}^{-1}$. These matrices are not calculated anew in each iteration, but $H_k$ or $B_k$ are updated to new matrices $H_{k+1}, B_{k+1} \in \mathbb{R}^{n \times n}$ using the information about the curvature of the objective function $f$ obtained during the iteration. It is required that matrices $H_{k+1}$ and $B_{k+1}$ generated by the update fulfill the so-called quasi-Newton equation, which reads as 
\begin{equation*}
    H_{k+1} (x_{k+1} - x_k) = \nabla f(x_{k+1}) - \nabla f(x_k) \quad \text{or} \quad B_{k+1} (\nabla f(x_{k+1}) - \nabla f(x_k)) = x_{k+1} - x_k.
\end{equation*}
For the sake of simplicity, we introduce the notations $s_k = x_{k+1} - x_k \in \mathbb{R}^n$ and $y_k = \nabla f(x_{k+1}) - \nabla f(x_k) \in \mathbb{R}^n$, thus we obtain
\begin{equation}\label{quasi-NewtonEquation}
    H_{k+1} s_k = y_k \quad \text{or} \quad B_{k+1} y_k = s_k.
\end{equation}
The fact that the matrices $H_{k+1}$ and $B_{k+1}$ satisfy the quasi-Newton equation, \cref{quasi-NewtonEquation}, is the distinguishing feature of quasi-Newton methods. \\
The idea now is to find a convenient formula, which requires at most the evaluation of the gradient $\nabla f(\cdot)$ of the objective function at each iterate, for updating the matrix $H_k$ or $B_k$ to a new matrix $H_{k+1}$ or $B_{k+1}$ that satisfies the quasi-Newton equation \cref{quasi-NewtonEquation}. Instead of recomputing $H_{k+1}$ or $B_{k+1}$ from scratch at every iteration, we apply a simple modification that combines the most recently observed information about the objective function with the existing knowledge embedded in $H_k$ or $B_k$ \cite[p.~139]{NocedalWright:2006}. \\
There are different update formulae, which of course differentiate the quasi-Newton methods. Probably the best-known method is based on the BFGS update, where the matrix $H^{\mathrm{BFGS}}_{k+1}$ or $B^{\mathrm{BFGS}}_{k+1}$ differs from its predecessor $H^{\mathrm{BFGS}}_k$ or $B^{\mathrm{BFGS}}_k$ by a rank-two matrix:
\begin{equation}\label{DirectBFGSformula}
    H^{\mathrm{BFGS}}_{k+1} = H^{\mathrm{BFGS}}_k + \frac{y_k y^{\mathrm{T}}_k}{s^{\mathrm{T}}_k y_k} - \frac{H^{\mathrm{BFGS}}_k s_k s^{\mathrm{T}}_k H^{\mathrm{BFGS}}_k}{s^{\mathrm{T}}_k H^{\mathrm{BFGS}}_k s_k}
\end{equation}
or 
\begin{equation}\label{InverseBFGSformula}
    B^{\mathrm{BFGS}}_{k+1} = \Big{(} I_{n \times n} - \frac{s_k y^{\mathrm{T}}_k}{s_k^{\mathrm{T}} y_k} \Big{)} B^{\mathrm{BFGS}}_k \Big{(} I_{n \times n} - \frac{y_k s^{\mathrm{T}}_k}{s_k^{\mathrm{T}} y_k} \Big{)} + \frac{s_k s^{\mathrm{T}}_k}{s_k^{\mathrm{T}} y_k}.
\end{equation}
If the matrix $H^{\mathrm{BFGS}}_k$ or $B^{\mathrm{BFGS}}_k$ is symmetric positive definite ($= \spd$), and the so-called curvature condition holds, which requires that 
\begin{equation}\label{CurvatureCondition}
    s^{\mathrm{T}}_k y_k > 0
\end{equation}
is satisfied, then $H^{\mathrm{BFGS}}_{k+1}$ or $B^{\mathrm{BFGS}}_{k+1}$ is also $\spd$ and satisfies the quasi-Newton equation \cref{quasi-NewtonEquation}. \\
But there is also a simpler rank-one update that maintains symmetry of the matrix $H_k$ or $B_k$ and satisfies the quasi-Newton equation \cref{quasi-NewtonEquation}. Unlike the rank-two BFGS update, this symmetric rank-one, or short SR1, update does not guarantee that the updated matrix $H_{k+1}$ or $B_{k+1}$ maintains positive definiteness. Nevertheless good numerical results have been obtained with algorithms based on SR1, therefore we now show a possible derivation of this formula, which is so simple that it has been rediscovered a number of times. \\
A general symmetric rank-one update has the form
\begin{equation*}
    H_{k+1} = H_k + \sigma \, v v^{\mathrm{T}},
\end{equation*}
where $v \in \mathbb{R}^n$ and $\sigma \in \{-1,1\}$. The task now is to determine $v$ and $\sigma$ so that $H_{k+1}$ satisfies the quasi-Newton equation, \cref{quasi-NewtonEquation}. By substituting into \cref{quasi-NewtonEquation}, we obtain
\begin{equation}\label{Derivation1}
    H_k s_k + [\sigma \, v^{\mathrm{T}} s_k] v = y_k.
\end{equation}
Since $[\sigma \, v^{\mathrm{T}} s_k]$ is a scalar, $v$ must be a multiple of $y_k − H_k s_k$, i.e. $v = \delta (y_k − H_k s_k)$ for some $\delta \in \mathbb{R}$. By substituting this form of $v$ into \cref{Derivation1}, we obtain
\begin{equation}\label{Derivation2}
    (y_k − H_k s_k) = \sigma \, \delta^2 \, [s^{\mathrm{T}}_k (y_k − H_k s_k)](y_k − H_k s_k),
\end{equation}
and it is clear that this equation is satisfied if (and only if) we choose the parameters $\delta$ and $\sigma$ to be
\begin{equation*}
    \sigma = \operatorname{sgn} (s^{\mathrm{T}}_k (y_k − H_k s_k)), \quad \delta = \pm \lvert s^{\mathrm{T}}_k (y_k − H_k s_k) \rvert^{-\frac{1}{2}}.
\end{equation*}
Hence, the only symmetric rank-one update formula that satisfies the quasi-Newton equation, \cref{quasi-NewtonEquation}, is given by
\begin{equation}\label{directSR1formula}
    H^\mathrm{SR1}_{k+1} = H^\mathrm{SR1}_k + \frac{(y_k - H^\mathrm{SR1}_k s_k) (y_k - H^\mathrm{SR1}_k s_k)^{\mathrm{T}}}{(y_k - H^\mathrm{SR1}_k s_k)^{\mathrm{T}} s_k}.
\end{equation}
By applying the Sherman–Morrison-Woodbury formula (cf. \cite[Theorem~1.2.16]{SunYuan:2006}), we obtain the corresponding update formula for the approximation of Hessian inverse ${\nabla^{2} f(x_{k+1})}^{-1}$:
\begin{equation}\label{inverseSR1formula}
    B^\mathrm{SR1}_{k+1} = B^\mathrm{SR1}_k + \frac{(s_k - B^\mathrm{SR1}_k y_k) (s_k - B^\mathrm{SR1}_k y_k)^{\mathrm{T}}}{(s_k - B^\mathrm{SR1}_k y_k)^{\mathrm{T}} y_k}.
\end{equation}
It is easy to see in that even if $H^\mathrm{SR1}_k$ is positive definite, $H^\mathrm{SR1}_{k+1}$ may not have the same property (this holds also for $B^\mathrm{SR1}_k$ and $B^\mathrm{SR1}_{k+1}$). If and only if $(y_k - H^\mathrm{SR1}_k s_k)^{\mathrm{T}} s_k > 0$, the SR1 update retains positive definiteness. However, this condition is difficult to guarantee. This means that $H^\mathrm{SR1}_{k+1}$ or $B^\mathrm{SR1}_{k+1}$ may no longer be invertible. Moreover, $d_{k+1} = -{H^\mathrm{SR1}_{k+1}}^{-1} \nabla f(x_{k+1}) = -B^\mathrm{SR1}_{k+1} \nabla f(x_{k+1})$ is not necessarily a descent direction. \\
The main drawback of the SR1 update formula is that the denominator in \cref{directSR1formula} or \cref{inverseSR1formula} can vanish. In fact, even when the objective function $f$ is convex and quadratic, there may be steps on which there is no symmetric rank-one update that satisfies the quasi-Newton equation \cref{quasi-NewtonEquation}. This disadvantage results in serious numerical difficulties, which restrict the applications of it. Nevertheless, the SR1 update formula has the following advantages:
\begin{enumerate}
    \item A simple safeguard seems to adequately prevent the breakdown of the method and the occurrence of numerical instabilities.
    \item The matrices generated by the SR1 formula, \cref{directSR1formula}, tend to be good approximations to the true Hessian matrix.
    \item In quasi-Newton methods for constrained problems, it may not be possible to impose \cref{CurvatureCondition}, and thus the BFGS update, \cref{DirectBFGSformula}, is not recommended. 
\end{enumerate}
For the vanishing denominator in \cref{directSR1formula} we introduce a strategy to prevent a method using the SR1 update from breaking down. It has been observed in practice that it performs well simply by skipping the update if the denominator is small. More specifically, the update \cref{directSR1formula} is applied only if 
\begin{equation}\label{CautiousSR1}
    \lvert (y_k - H^\mathrm{SR1}_k s_k)^{\mathrm{T}} s_k \lvert \; \geq \; r \; \lVert s_k \rVert \lVert y_k - H^\mathrm{SR1}_k s_k \rVert 
\end{equation}
holds, where $r \in (0, 1)$ is a small number, e.g. $r = 10^{−8}$. Most implementations of the SR1 update use a skipping rule of this kind. The condition $(y_k - H^\mathrm{SR1}_k s_k)^{\mathrm{T}} s_k \approx 0$ occurs infrequently, since it requires certain vectors to be aligned in a specific way. When it occurs, skipping the update appears to have no negative effects on the iteration, since the skipping condition implies that $s^{\mathrm{T}}_k \tilde{G} s_k \approx s^{\mathrm{T}}_k H^\mathrm{SR1}_k s_k$, where $\tilde{G}$ is the average Hessian over the last step, meaning that the curvature of $H^\mathrm{SR1}_k$ along $s_k$ is already correct. \\
In summary, a general quasi-Newton method using the inverse SR1 update, \cref{inverseSR1formula}, is as follows:
\begin{algorithm}[H]
    \caption{Inverse SR1 Method}\label{InverseSR1Method}
    \begin{algorithmic}[1]
        \State Continuously differentiable real-valued function $f$ on $\mathbb{R}^n$, bounded below; initial iterate $x_0 \in \mathbb{R}^n$; initial $\spd$ matrix $B^{\mathrm{SR1}}_0 \in \mathbb{R}^{n \times n}$; convergence tolerance $\varepsilon > 0$. Set $k = 0$.
        \While{$\lVert \nabla f(x_k) \rVert > \varepsilon$}
            \State Compute the search direction $d_k = - B^{\mathrm{SR1}}_k \nabla f(x_k)$.
            \State Determine a suitable stepsize $\alpha_k > 0$.
            \State Set $x_{k+1} = x_k + \alpha_k d_k$.
            \State Set $s_k = x_{k+1} - x_k$ and $y_k = \nabla f(x_{k+1}) - \nabla f(x_k)$.
            \State Compute $B^{\mathrm{SR1}}_{k+1} \in \mathbb{R}^{n \times n}$ by means of \cref{inverseSR1formula}. 
            \State Set $k = k+1$.
        \EndWhile
        \State \textbf{Return} $x_k$.
    \end{algorithmic}
\end{algorithm}
One of the main advantages of the SR1 method is its ability to generate good Hessian approximations. We first consider the case of a quadratic objective function. We have the following statement:
\begin{theorem}[{\cite[Theorem~6.1.]{NocedalWright:2006}}]
    Suppose that $f \colon \; \mathbb{R}^n \to \mathbb{R}$ is the strongly convex quadratic function $f(x) = \frac{1}{2} x^{\mathrm{T}} A x + b^{\mathrm{T}} x$, where $A \in \mathbb{R}^{n \times n}$ is symmetric positive definite. Then for any starting point $x_0$ and any symmetric starting matrix $B^{\mathrm{SR1}}_0 \in \mathbb{R}^{n \times n}$, the iterates $\{x_k\}_k$ generated by \cref{InverseSR1Method} with constant stepsize $\alpha_k = 1$ converge to the minimizer in at most $n$ steps, provided that $(s_k - B^\mathrm{SR1}_k y_k)^{\mathrm{T}} y_k \neq 0$ for all $k$. Moreover, if $n$ steps are performed, and if the search directions $d_k$ are linearly independent, then $B^{\mathrm{SR1}}_n = A^{−1}$.
\end{theorem}
The distinguishing feature of the SR1 quasi-Newton method with constant stepsize is its natural quadratic termination, which means that for a quadratic function, the method terminates within $n$ steps. \\
In the proof it is shown that the SR1 update possesses the so-called hereditary property, i.e. 
\begin{equation}\label{HereditaryProperty}
    B^{\mathrm{SR1}}_k y_i = s_i, \quad i = 0, \cdots, k-1.
\end{equation}
This relation shows that when $f$ is quadratic, the quasi-Newton equation, \cref{quasi-NewtonEquation}, is satisfied along all previous search directions, regardless of how the line search is performed. A result like this can be established for the BFGS update, \cref{InverseBFGSformula}, only under the restrictive assumption that an exact line search is used. \\
For general nonlinear functions, the SR1 update continues to generate good Hessian approximations under certain conditions:
\begin{theorem}[{\cite[Theorem~6.2.]{NocedalWright:2006}}]
    Suppose that $f$ is twice continuously differentiable, and that its Hessian is bounded and Lipschitz continuous in a neighborhood of a point $x^{*}$. Let $\{x_k\}_k$ be any sequence of iterates such that $x_k \rightarrow x^{*}$ for some $x^{*} \in \mathbb{R}^n$. Suppose in addition that the inequality \cref{CautiousSR1} holds for all $k$, for some $r \in (0,1)$, and that the steps $s_k$ are uniformly linearly independent. Then the matrices $H^{\mathrm{SR1}}_k$ generated by \cref{directSR1formula} satisfy
    \begin{equation*}
        \lim_{k \to \infty} \lVert H^{\mathrm{SR1}}_k - \nabla^2 f(x^{*}) \rVert = 0.
    \end{equation*}
\end{theorem}
“Uniformly linearly independent steps” means, that the steps do not tend to fall in a subspace of a dimension less than $n$. This assumption is usually, but not always, satisfied in practice \cite[p.~144-149]{NocedalWright:2006}. \\
The convergence properties of the SR1 quasi-Newton method are not as well understood as those of the BFGS method. No global results or local superlinear results have been established, except the mentioned results for quadratic functions.