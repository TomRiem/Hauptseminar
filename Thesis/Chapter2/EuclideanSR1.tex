\chapter{Euclidean Trust-Region Symmetric Rank-One Method}

In the Euclidean optimization a key problem is minimizing a real-valued function $f$ over the Euclidean space $\mathbb{R}^n$ ($n \geq 1$), i.e. our focus and efforts are centred on solving 
\begin{equation}\label{OptimizationProblem}
    \min f(x), \quad x \in \mathbb{R}^n
\end{equation}  
where $f \colon \; \mathbb{R}^n \to \mathbb{R}$ is a smooth function. In this chapter we focus on smooth functions, by which we generally mean functions whose second derivatives exist and are continuous or formally $f \in C^2(\mathbb{R}^n)$, unless otherwise stated. \cref{OptimizationProblem} is called a (nonlinear) unconstrained optimization problem. \\

The matrix $H_k \in \mathbb{R}^{n \times n}$ approximates the action of the objective's Hessian $\nabla^{2} f(\cdot)$ at the current iterate $x_k$ in the direction of $s_k$. These matrices are not calculated anew in each iteration, but $H_k$ is updated to a new matrix $H_{k+1} \in \mathbb{R}^{n \times n}$ using the information about the curvature of the objective function $f$ obtained during the iteration. It is required that the matrix $H_{k+1}$ generated by the update fulfills the so-called quasi-Newton equation, which reads as 
\begin{equation*}
    H_{k+1} (x_{k+1} - x_k) = \nabla f(x_{k+1}) - \nabla f(x_k).
\end{equation*}
For the sake of simplicity, we introduce the notations $s_k = x_{k+1} - x_k \in \mathbb{R}^n$ and $y_k = \nabla f(x_{k+1}) - \nabla f(x_k) \in \mathbb{R}^n$, thus we obtain
\begin{equation}\label{quasi-NewtonEquation}
    H_{k+1} s_k = y_k.
\end{equation}
The fact that the matrix $H_{k+1}$ satisfies the quasi-Newton equation, \cref{quasi-NewtonEquation}, is the distinguishing feature of quasi-Newton methods. \\
The idea now is to find a convenient formula, which requires at most the evaluation of the gradient $\nabla f(\cdot)$ of the objective function at each iterate, for updating the matrix $H_k$ to a new matrix $H_{k+1}$, which satisfies the quasi-Newton equation \cref{quasi-NewtonEquation}. Instead of recomputing $H_{k+1}$ from scratch at every iteration, we apply a simple modification that combines the most recently observed information about the objective function with the existing knowledge embedded in $H_k$ or $B_k$ \cite[p.~139]{NocedalWright:2006}. \\

But there is also a simpler rank-one update that maintains symmetry of the matrix $H_k$ or $B_k$ and satisfies the quasi-Newton equation \cref{quasi-NewtonEquation}. Unlike the rank-two BFGS update, this symmetric rank-one, or short SR1, update does not guarantee that the updated matrix $H_{k+1}$ or $B_{k+1}$ maintains positive definiteness. Nevertheless good numerical results have been obtained with algorithms based on SR1, therefore we now show a possible derivation of this formula, which is so simple that it has been rediscovered a number of times. \\
A general symmetric rank-one update has the form
\begin{equation*}
    H_{k+1} = H_k + \sigma \, v v^{\mathrm{T}},
\end{equation*}
where $v \in \mathbb{R}^n$ and $\sigma \in \{-1,1\}$. The task now is to determine $v$ and $\sigma$ so that $H_{k+1}$ satisfies the quasi-Newton equation, \cref{quasi-NewtonEquation}. By substituting into \cref{quasi-NewtonEquation}, we obtain
\begin{equation}\label{Derivation1}
    H_k s_k + [\sigma \, v^{\mathrm{T}} s_k] v = y_k.
\end{equation}
Since $[\sigma \, v^{\mathrm{T}} s_k]$ is a scalar, $v$ must be a multiple of $y_k − H_k s_k$, i.e. $v = \delta (y_k − H_k s_k)$ for some $\delta \in \mathbb{R}$. By substituting this form of $v$ into \cref{Derivation1}, we obtain
\begin{equation}\label{Derivation2}
    (y_k − H_k s_k) = \sigma \, \delta^2 \, [s^{\mathrm{T}}_k (y_k − H_k s_k)](y_k − H_k s_k),
\end{equation}
and it is clear that this equation is satisfied if (and only if) we choose the parameters $\delta$ and $\sigma$ to be
\begin{equation*}
    \sigma = \operatorname{sgn} (s^{\mathrm{T}}_k (y_k − H_k s_k)), \quad \delta = \pm \lvert s^{\mathrm{T}}_k (y_k − H_k s_k) \rvert^{-\frac{1}{2}}.
\end{equation*}
Hence, the only symmetric rank-one update formula that satisfies the quasi-Newton equation, \cref{quasi-NewtonEquation}, is given by
\begin{equation}\label{directSR1formula}
    H^\mathrm{SR1}_{k+1} = H^\mathrm{SR1}_k + \frac{(y_k - H^\mathrm{SR1}_k s_k) (y_k - H^\mathrm{SR1}_k s_k)^{\mathrm{T}}}{(y_k - H^\mathrm{SR1}_k s_k)^{\mathrm{T}} s_k}.
\end{equation}

It is easy to see in that even if $H^\mathrm{SR1}_k$ is positive definite, $H^\mathrm{SR1}_{k+1}$ may not have the same property (this holds also for $B^\mathrm{SR1}_k$ and $B^\mathrm{SR1}_{k+1}$). If and only if $(y_k - H^\mathrm{SR1}_k s_k)^{\mathrm{T}} s_k > 0$, the SR1 update retains positive definiteness. However, this condition is difficult to guarantee. This means that $H^\mathrm{SR1}_{k+1}$ or $B^\mathrm{SR1}_{k+1}$ may no longer be invertible. Moreover, $d_{k+1} = -{H^\mathrm{SR1}_{k+1}}^{-1} \nabla f(x_{k+1}) = -B^\mathrm{SR1}_{k+1} \nabla f(x_{k+1})$ is not necessarily a descent direction. \\
The main drawback of the SR1 update formula is that the denominator in \cref{directSR1formula} can vanish. In fact, even when the objective function $f$ is convex and quadratic, there may be steps on which there is no symmetric rank-one update that satisfies the quasi-Newton equation \cref{quasi-NewtonEquation}. This disadvantage results in serious numerical difficulties, which restrict the applications of it. Nevertheless, the SR1 update formula has the following advantages:
For the vanishing denominator in \cref{directSR1formula} we introduce a strategy to prevent a method using the SR1 update from breaking down. It has been observed in practice that it performs well simply by skipping the update if the denominator is small. More specifically, the update \cref{directSR1formula} is applied only if 
\begin{equation}\label{safeguard}
    \lvert (y_k - H^\mathrm{SR1}_k s_k)^{\mathrm{T}} s_k \lvert \; \geq \; \nu \; \lVert s_k \rVert \lVert y_k - H^\mathrm{SR1}_k s_k \rVert 
\end{equation}
holds, where $\nu \in (0, 1)$ is a small number, e.g. $r = 10^{−8}$. Most implementations of the SR1 update use a skipping rule of this kind. The condition $(y_k - H^\mathrm{SR1}_k s_k)^{\mathrm{T}} s_k \approx 0$ occurs infrequently, since it requires certain vectors to be aligned in a specific way. When it occurs, skipping the update appears to have no negative effects on the iteration, since the skipping condition implies that $s^{\mathrm{T}}_k \tilde{G} s_k \approx s^{\mathrm{T}}_k H^\mathrm{SR1}_k s_k$, where $\tilde{G}$ is the average Hessian over the last step, meaning that the curvature of $H^\mathrm{SR1}_k$ along $s_k$ is already correct. \\

\begin{equation}\label{trsubproblem}
    s_k = \arg \min_{\lVert s \rVert_2 \leq \Delta_k} m_k(s) = \arg \min_{\lVert s \rVert_2 \leq \Delta_k} f(x_k) + \nabla f(x_k)^{\mathrm{T}} s + \frac{1}{2} s^{\mathrm{T}} H_k s.
\end{equation}

\begin{algorithm}[H]
    \caption{Trust-Region Symmetric Rank-One Method}\label{TR-SR1Method}
    \begin{algorithmic}[1]
        \State Continuously differentiable real-valued function $f$ on $\mathbb{R}^n$, bounded below; initial iterate $x_0 \in \mathbb{R}^n$; initial $\spd$ matrix $H^{\mathrm{SR1}}_0 \in \mathbb{R}^{n \times n}$; initial trust-region radius $\Delta_0 > 0$; safeguard constant $\nu \in (0,1)$; rejection boundary $\rho^{\prime} \in (0, 0.1)$; trust-region reduction factor $\tau_1 \in (0,1)$; trust-region magnification factor $\tau_2 > 1$; convergence tolerance $\varepsilon > 0$. Set $k = 0$.
        \While{$\lVert \nabla f(x_k) \rVert > \varepsilon$}
            \State Obtain $s_k$ (approximately) solving \cref{trsubproblem}.
            \State Set $\widetilde{x}_{k+1} = x_k + s_k$ and $y_k = \nabla f(\widetilde{x}_{k+1}) - \nabla f(x_k)$.
            \If{\cref{safeguard} holds}
                \State Compute $H^{\mathrm{SR1}}_{k+1} \in \mathbb{R}^{n \times n}$ by means of \cref{directSR1formula}.
			\Else 
				\State Set $H^{\mathrm{SR1}}_{k+1} = H^{\mathrm{SR1}}_k$.
            \EndIf 
            \State Compute $\rho_k = \frac{f(x_k) - f(\widetilde{x}_{k+1})}{m_k(0) - m_k(s_k)}$.
            \If{$\rho_k > \rho^{\prime}$}
                \State Set $x_{k+1} = \widetilde{x}_{k+1}$.
			\Else 
				\State Set $x_{k+1} = x_k$.
            \EndIf 
            \If{$\rho_k > 0.75$}
                \If{$\lVert s_k \rVert \geq 0.8 \ \Delta_k$}
                    \State Set $\Delta_{k+1} = \tau_2 \ \Delta_k$.
                \Else 
                    \State Set $\Delta_{k+1} = \Delta_k$.
                \EndIf 
			\Else 
                \If{$\rho_k < 0.1$}
                    \State Set $\Delta_{k+1} = \tau_1 \ \Delta_k$.
                \Else 
                    \State Set $\Delta_{k+1} = \Delta_k$.
                \EndIf 
            \EndIf 
            \State Set $k = k+1$.
        \EndWhile
        \State \textbf{Return} $x_k$.
    \end{algorithmic}
\end{algorithm}


\begin{assumption}[{\cite[assumptions~(A1)+(A3)]{ByrdKhalfanSchnabel:1996}}]\label{AssumptionsGlobalConvergence} \ \\[-1.5\baselineskip]
    \begin{enumerate}
        \item The sequence of iterates does not terminate and remains in a closed, bounded, convex set $D$ on which the function $f$ is twice continuously differentiable and in which $f$ has a unique stationary point $x^*$, i.e. $\nabla f(x^*) = 0$. The Hessian $\nabla^2 f(x^*)$ is positive definite, and $\nabla^2 f(x)$ is Lipschitz continuous in a neighborhood of $x^*$; that is, there exists a constant $\gamma > 0$ such that for all $x, y$ in some neighborhood of $x^*$ \begin{equation*} \lVert \nabla^2 f(x) - \nabla^2 f(y) \rVert \geq \gamma \ \lVert x - y \rVert. \end{equation*}
        \item The sequence of matrices $\{ H^{\mathrm{SR1}}_k \}_k$ is bounded by a constant $M$ such that $\lVert H^{\mathrm{SR1}}_k \rVert \leq M$ for all $k$.
    \end{enumerate}
\end{assumption}

\begin{theorem}[{\cite[Theorem~2.1.]{ByrdKhalfanSchnabel:1996}}] \label{GlobalConvergence}
    If the sequence $\{ x_k \}_k$ is generated by \cref{TR-SR1Method} and \cref{AssumptionsGlobalConvergence} holds, then $x_k \rightarrow x^*$.
\end{theorem}

\begin{assumption}[{\cite[assumptions~(A1)+(A3)]{ByrdKhalfanSchnabel:1996}}]\label{AssumptionsLocalConvergence} \ \\[-1.5\baselineskip]
    \begin{enumerate}
        \item The sequence of matrices $\{ H^{\mathrm{SR1}}_k \}_k$ is generated from each iterate $x_k$ by \cref{directSR1formula}, using $s_k$, and for each iteration \cref{safeguard} holds, where $\nu \in (0, 1)$ is a constant.
        \item The trust-region subproblem \cref{trsubproblem} is solved accurately enough that for all $k$ \begin{equation*} -(\nabla f(x_k)^{\mathrm{T}} s + \frac{1}{2} s^{\mathrm{T}} H^{\mathrm{SR1}}_k s) \geq \sigma_1 \ \lVert \nabla f(x_k) \rVert \ \min \{ \Delta_k, \sigma_2 \ \frac{\lVert \nabla f(x_k) \rVert}{\lVert H^{\mathrm{SR1}}_k \rVert} \} \end{equation*} for positive constants $\sigma_1$ and $\sigma_2$ and such that whenever $\lVert s_k \rVert < 0.8 \ \Delta_k$, then $H^{\mathrm{SR1}}_k s_k = - \nabla f(x_k)$.
    \end{enumerate}
\end{assumption}

\begin{theorem}[{\cite[Theorem~2.7.]{ByrdKhalfanSchnabel:1996}}] \label{LocalConvergence}
    Consider \cref{TR-SR1Method} and suppose that \cref{AssumptionsGlobalConvergence} and \cref{AssumptionsLocalConvergence} hold. Then the sequence $\{ x_k \}_k$ generated by \cref{TR-SR1Method} converges $n+l$-step superlinear, i.e. 
    \begin{equation}\label{n+1superlinear}
        \lim_{k \rightarrow \infty} \frac{\lVert x_{k+n+1} - x^* \rVert}{\lVert x_k - x^* \rVert} = 0.
    \end{equation}
\end{theorem}